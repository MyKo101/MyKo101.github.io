[{"authors":["admin"],"categories":null,"content":"Dr. Michael Barrowman is a Data Scientist, currently employed by Evolution Sofware, a company which manages online casinos and builds their internal software structure. With a background in both the public and private sector and a wealth of experience working with medical, education and financial datasets; he has produced stunning visualisations for previous teams as well as publication-ready reports and production-ready scripts.\nHe has recently completed his doctoral Thesis on Multi-State Clinical Prediction Models in Renal Replacement Therapy, earning him a PhD from the University of Manchester. His PhD project encompasses the development and validation of a multi-state clinical predication model, as well as the methodological advancements to produce such a model. This has led to multiple publications and the creation of software as a by-product.\nHe is interested in Data Science, particularly using R and python to their fullest potential, encouraging others to do the same and is an advocate for neat and reproducible coding practices. He enjoys web development and learning about web paradigms as wel as learning new programming languages such as C++ and Typescript.\nHe lives in Merseyside, UK with his partner, two children and two step-children. He enjoys walks down by the local canal, through nearby forested areas and trips to the park as often as possible as his daughter\u0026rsquo;s favourite outdoor activity is \u0026ldquo;going on adventures\u0026rdquo;.\n","date":1595721600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1595721600,"objectID":"8c1425bb4f5a5b836c1d93a1a8e2f00e","permalink":"https://michaelbarrowman.co.uk/author/dr.-michael-barrowman-phd/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/dr.-michael-barrowman-phd/","section":"authors","summary":"Dr. Michael Barrowman is a Data Scientist, currently employed by Evolution Sofware, a company which manages online casinos and builds their internal software structure. With a background in both the public and private sector and a wealth of experience working with medical, education and financial datasets; he has produced stunning visualisations for previous teams as well as publication-ready reports and production-ready scripts.","tags":null,"title":"Dr. Michael Barrowman, PhD","type":"authors"},{"authors":null,"categories":null,"content":"","date":1592784e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592784e3,"objectID":"2794fcc40e76c39c475a81d1ff126c38","permalink":"https://michaelbarrowman.co.uk/package/mpipe/","publishdate":"2020-06-22T00:00:00Z","relpermalink":"/package/mpipe/","section":"package","summary":"The mpipe package is designed to add extra functionality to the pipeline process in tidyverse style R usage","tags":["R Package"],"title":"mpipe","type":"package"},{"authors":null,"categories":null,"content":"","date":1595289600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595289600,"objectID":"3c651fb0165c246151d4f319ece9b8e6","permalink":"https://michaelbarrowman.co.uk/package/mutils/","publishdate":"2020-07-21T00:00:00Z","relpermalink":"/package/mutils/","section":"package","summary":"The goal of mutils is to provide useful functions to make data processing smoother. Most functions contained here are 'nifty', rather than 'innovative'.","tags":["R Package"],"title":"mutils","type":"package"},{"authors":null,"categories":null,"content":"","date":1610928e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610928e3,"objectID":"1f3ffbc1bfa199b529d032491bd5d535","permalink":"https://michaelbarrowman.co.uk/package/rando/","publishdate":"2021-01-18T00:00:00Z","relpermalink":"/package/rando/","section":"package","summary":"The goal of rando is to provide easier generating of random numbers in a manner that is context aware, and reproducible.","tags":["R Package"],"title":"rando","type":"package"},{"authors":null,"categories":null,"content":"","date":1592092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592092800,"objectID":"b9e4cd6f5a58b4a8277e294b4b3e1aab","permalink":"https://michaelbarrowman.co.uk/package/typos/","publishdate":"2020-06-14T00:00:00Z","relpermalink":"/package/typos/","section":"package","summary":"The goal of typos is to provide a flexible warning when commonly mis-typed functions are called. Functions with typing errors will still be evaluated and a warning will be output. It also provides the user with a convenient function to define their own typos.","tags":["R Package"],"title":"typos","type":"package"},{"authors":[],"categories":[],"content":"  This question on Reddit, got me thinking about the lapply() family of functions, and how a beginner might want to learn about them. Here is my take\nIntroduction The easiest one to understand is lapply(), I’ll work through that and then extend to the others. As an aside, the programmatic terminology is vectorising as it allows us to perform an action over an entire vector at once or list in R.\nIgnoring the dots, lapply() takes two arguments X and FUN. FUN is the name of the function, and X is a list of objects. When I say list, this could be an actual, as created by the list() function, or a vector such as 1:10. But if you try to put something more complicated in like a data.frame(), you can get unexpected results (I’ll come back to this).\n lists So, let’s say we have\nX \u0026lt;- list(1:10,11:20,21:30) X ## [[1]] ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## [[2]] ## [1] 11 12 13 14 15 16 17 18 19 20 ## ## [[3]] ## [1] 21 22 23 24 25 26 27 28 29 30 This list has three elements, and each element consists of a vector of 10 numbers. We can access them using [[, where X[[1]] will return the first element, the numbers 1 to 10:\nX[[1]] ## [1] 1 2 3 4 5 6 7 8 9 10 X[[2]] will return the second element, etc… This is extraction as it extracts an element from a list. Extraction can only bring out a single element. We can also subset using [ for example\nX[1:2] ## [[1]] ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## [[2]] ## [1] 11 12 13 14 15 16 17 18 19 20 This return the first and second elements. X[1] will return a subset consisting of the first element.\nX[1] ## [[1]] ## [1] 1 2 3 4 5 6 7 8 9 10 What’s the difference between X[1] and X[[1]]? Well, X[1] returns a list, which is just 1 element long, that element being a vector of the numbers from 1 to 10. X[[1]] returns the actual element at position 1.\nlength(X[1]) ## [1] 1 length(X[[1]]) ## [1] 10 class(X[1]) ## [1] \u0026quot;list\u0026quot; class(X[[1]]) ## [1] \u0026quot;integer\u0026quot; So X[1] is a list, just like X but is shorter, a subset, just like how X[1:2] is a subset with length 2. Whereas X[[1]] is the first element of X. This is clearer if we try to add something to these two objects:\nX[[1]] + 3 ## [1] 4 5 6 7 8 9 10 11 12 13 X[1] + 3 ## Error in X[1] + 3: non-numeric argument to binary operator Again, to stress the point. X[1] is not a number, it is a list containing a single element. Since X[1] is a list, we can therefore extract that first element from it:\nX[1][[1]] ## [1] 1 2 3 4 5 6 7 8 9 10 The difference between a list and a vector is that a vector has to all be of the same type (e.g. all characters as in c(\"a\",\"b\",\"c\") or all numbers as in c(1,2,3), the c() function will coerce them otherwise, so c(1,\"2\",3) will coerce to characters. But a list can all be different, so list(\"hello\",2,1:10) has three elements. In fact lists can contain lists (nested lists)\nY \u0026lt;- list(\u0026quot;hello\u0026quot;,1:10,list(\u0026quot;one\u0026quot;,\u0026quot;two\u0026quot;,\u0026quot;three\u0026quot;)) Y ## [[1]] ## [1] \u0026quot;hello\u0026quot; ## ## [[2]] ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## [[3]] ## [[3]][[1]] ## [1] \u0026quot;one\u0026quot; ## ## [[3]][[2]] ## [1] \u0026quot;two\u0026quot; ## ## [[3]][[3]] ## [1] \u0026quot;three\u0026quot; has three elements. If you extract the third element,\nY[[3]] ## [[1]] ## [1] \u0026quot;one\u0026quot; ## ## [[2]] ## [1] \u0026quot;two\u0026quot; ## ## [[3]] ## [1] \u0026quot;three\u0026quot; you get another list. If you subset the third element,\nY[3] ## [[1]] ## [[1]][[1]] ## [1] \u0026quot;one\u0026quot; ## ## [[1]][[2]] ## [1] \u0026quot;two\u0026quot; ## ## [[1]][[3]] ## [1] \u0026quot;three\u0026quot; you get a list with 1 element.\nAs far as nomenclature is concerned, a vector is a type of list which has the requirement that all entries be of the same type. You can even use extraction on a vector,\nx \u0026lt;- 1:10 x[3] ## [1] 3 x[[3]] ## [1] 3 Although when dealing with a vector, the second version is much less common. The reason this works is that extraction and subsetting are essentially the same thing in a vector (because it will always return a vector, it just might be of length 1).\n lapply() So, now that we know what a list is, we can look at what lapply() does to that list. If we supply a function, lapply() will run that function on every element in that list. The simplest example would be, using X from above, lapply(X,mean) will return a list with the mean() of every element in X.\nlapply(X,mean) ## [[1]] ## [1] 5.5 ## ## [[2]] ## [1] 15.5 ## ## [[3]] ## [1] 25.5 Remember that the elements in X are the vectors of numbers, 1:10, 11:20 and 21:30. We’ve applied the function to the list a list-apply.\nThe function doesn’t have to be one that is named, and we can supply a function in-line\nlapply(X, function(x) mean(x-5.5)) ## [[1]] ## [1] 0 ## ## [[2]] ## [1] 10 ## ## [[3]] ## [1] 20 This applies the function function(x) mean(x-5.5) to every element in X. You could define this function outside of the lapply() function earlier, but there is no need if this is the only place we plan on using it.\nFor future, when the R 4.1 version is released, I believe this will be even easier with the shorthand \\() syntax.\nlapply(X, \\(x) mean(x - 5.5)) So running lapply(X,FUN) is the same as running the following for() loop\noutput \u0026lt;- vector(\u0026quot;list\u0026quot;,length(X)) for(i in 1:length(X)){ output[[i]] \u0026lt;- FUN(X[[i]]) } Compare the previous code to this:\noutput \u0026lt;- vector(\u0026quot;list\u0026quot;,length(X)) for(i in 1:length(X)){ output[[i]] \u0026lt;- mean(X[[i]]) } output ## [[1]] ## [1] 5.5 ## ## [[2]] ## [1] 15.5 ## ## [[3]] ## [1] 25.5 Notice that I’ve defined the output \u0026lt;- vector(\"list\",length(X)) before running the for() loop. This line basically makes an empty list of the defined length. This will come up when we move on from lapply()\n dots One part of lapply() that I’ve ignored is the ... dots argument. These are basically other arguments that you want passed on to your function. Whatever is in the dots, will be passed to every call to FUN, whether named or not:\nlapply(c(\u0026quot;a\u0026quot;,\u0026quot;b\u0026quot;,\u0026quot;c\u0026quot;),paste,\u0026quot;2\u0026quot;) ## [[1]] ## [1] \u0026quot;a 2\u0026quot; ## ## [[2]] ## [1] \u0026quot;b 2\u0026quot; ## ## [[3]] ## [1] \u0026quot;c 2\u0026quot; lapply(list( 1:10, c(1,2,NA,4), 21:30), mean, na.rm=T) ## [[1]] ## [1] 5.5 ## ## [[2]] ## [1] 2.333333 ## ## [[3]] ## [1] 25.5 Essentially, this runs the following loop:\nX \u0026lt;- list( 1:10, c(1,2,NA,4), 21:30) output \u0026lt;- vector(\u0026quot;list\u0026quot;,3) for(i in 1:3){ output[[i]] \u0026lt;- mean(X[[i]],na.rm=T) } output ## [[1]] ## [1] 5.5 ## ## [[2]] ## [1] 2.333333 ## ## [[3]] ## [1] 25.5 Hopefully that will be enough to understand lapply(). One unusual case is using lapply() on a data.frame-like structure. Now, a data.frame looks like a table, but it’s actually a list, but the list is counter intuitive. Each element of the list is a column in the data.frame. So, if you run the following, you would get a result that is only 4 elements long\niris0 \u0026lt;- iris[,1:4] lapply(iris0,mean) ## $Sepal.Length ## [1] 5.843333 ## ## $Sepal.Width ## [1] 3.057333 ## ## $Petal.Length ## [1] 3.758 ## ## $Petal.Width ## [1] 1.199333 You might think that this would work across the rows of the data.frame, but it works down the columns. Also note that these outputs are now also named the same as the input list. This can be useful for keeping track of your inputs and outputs.\n apply() This brings us only apply().\nThe apply() function does a similar job, however it doesn’t work on lists, it works on multi-dimensional objects, so matrices and arrays. It tries to collapse a multi-dimensional object down by one (or more) of its dimensions. So it turns a matrix into a vector (or an array into a smaller array). As well as X (which must be multi-dimensional, so definitely not a list) and FUN, it also takes MARGIN which tells apply() which dimension(s) to collapse:\nM \u0026lt;- matrix(1:9,nrow=3) apply(M,1,mean) #takes the mean of each row ## [1] 4 5 6 apply(M,2,mean) #takes the mean of each column ## [1] 2 5 8 The type returned is the same as the type we started with, and once again apply() can take other arguments as dots. So this works quite well with character matrices: {\nM \u0026lt;- matrix(letters[1:9],nrow=3) apply(M,1,paste0,collapse=\u0026quot;\u0026quot;) #pastes across the rows ## [1] \u0026quot;adg\u0026quot; \u0026quot;beh\u0026quot; \u0026quot;cfi\u0026quot; apply(M,2,paste0,collapse=\u0026quot;\u0026quot;) #pastes down the columns ## [1] \u0026quot;abc\u0026quot; \u0026quot;def\u0026quot; \u0026quot;ghi\u0026quot; This means we can use apply() on a data.frame to work across the rows, rather than down the columns. In this case, ever though a data.frame is a list, because it can be accessed in the same way as a matrix, it still works\napply(iris0,1,mean) ## [1] 2.550 2.375 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 2.700 2.500 ## [13] 2.325 2.125 2.800 3.000 2.750 2.575 2.875 2.675 2.675 2.675 2.350 2.650 ## [25] 2.575 2.450 2.600 2.600 2.550 2.425 2.425 2.675 2.725 2.825 2.425 2.400 ## [37] 2.625 2.500 2.225 2.550 2.525 2.100 2.275 2.675 2.800 2.375 2.675 2.350 ## [49] 2.675 2.475 4.075 3.900 4.100 3.275 3.850 3.575 3.975 2.900 3.850 3.300 ## [61] 2.875 3.650 3.300 3.775 3.350 3.900 3.650 3.400 3.600 3.275 3.925 3.550 ## [73] 3.800 3.700 3.725 3.850 3.950 4.100 3.725 3.200 3.200 3.150 3.400 3.850 ## [85] 3.600 3.875 4.000 3.575 3.500 3.325 3.425 3.775 3.400 2.900 3.450 3.525 ## [97] 3.525 3.675 2.925 3.475 4.525 3.875 4.525 4.150 4.375 4.825 3.400 4.575 ## [109] 4.200 4.850 4.200 4.075 4.350 3.800 4.025 4.300 4.200 5.100 4.875 3.675 ## [121] 4.525 3.825 4.800 3.925 4.450 4.550 3.900 3.950 4.225 4.400 4.550 5.025 ## [133] 4.250 3.925 3.925 4.775 4.425 4.200 3.900 4.375 4.450 4.350 3.875 4.550 ## [145] 4.550 4.300 3.925 4.175 4.325 3.950 now gives a vector of the averages of each row.\n Rest of the family Now for lapply()’s sisters:\nvapply() takes an extra argument, which is of the same type as what you want your outcome to be. This is the one that I use most often. You can think of it like a lapply() that will output something other than a list. I usually give FUN.VALUE as something like integer(1) or character(1). These functions generate empty vectors of that type, they are wrappers around things like vector(\"integer\",1)\nX \u0026lt;- list(1:10,11:20,21:30) vapply(X,mean,numeric(1)) ## [1] 5.5 15.5 25.5 This time, we get a numeric vector, rather than a list like we would with lapply(). I find this much easier to ensure I’m working with the correct type of data.\nsapply() tries to simplify your output, So if lapply() outputs a list of vectors that are all the same length, instead of a list, it’ll return a matrix\nX \u0026lt;- list(1:5, 6:10, 11:15) sapply(X,range) ## [,1] [,2] [,3] ## [1,] 1 6 11 ## [2,] 5 10 15 Each column in this result is the same as one of the elements of the list lapply(X,range). They’ve just been cbind’d together. The use of sapply() is not common as the output can be inconsistent. `vapply() is much prefered as it gives more control over the output. The above can be replicated with vapply() and will throw an error if the output is unexpected:\nX \u0026lt;- list(1:5, 6:10, 11:15) vapply(X,range,numeric(2)) ## [,1] [,2] [,3] ## [1,] 1 6 11 ## [2,] 5 10 15 tapply() is more complicated as it subsets the X based on the INDEX. It describes this as a “Ragged Array”. I have never used this directly, as I will usually do the subsetting manually using split(), but that is essentialy what tapply() does behind the scenes. tapply() also comes with a simplify argument, which decides whether R will try and simplify the results, like in sapply() or not, by default it will try and invoke this simplification. The following are therefore (roughly) equivalent\ntapply(X, INDEX, FUN, simplify=FALSE) lapply(split(X,INDEX), FUN) split() creates a list where the first vector is split into groups based on the second argument.\nSo we can compare using both a lapply() and a vapply()\nx \u0026lt;- 1:10 grp \u0026lt;- c(1,1,1,2,2,3,3,3,4,5) tapply(x,grp,sum,simplify=FALSE) ## $`1` ## [1] 6 ## ## $`2` ## [1] 9 ## ## $`3` ## [1] 21 ## ## $`4` ## [1] 9 ## ## $`5` ## [1] 10 lapply(split(x,grp),sum) ## $`1` ## [1] 6 ## ## $`2` ## [1] 9 ## ## $`3` ## [1] 21 ## ## $`4` ## [1] 9 ## ## $`5` ## [1] 10 tapply(x,grp,sum) ## 1 2 3 4 5 ## 6 9 21 9 10 vapply(split(x,grp),sum,numeric(1)) ## 1 2 3 4 5 ## 6 9 21 9 10 The other member of the lapply() family is mapply(). This is even more powerful as it allows you to vectorise over multiple arguments, rather than just the first. Syntactically, the difference here is that the dots are the vectorised arguments, and the non-vectorised arguments go into the MoreArgs argument.\nX \u0026lt;- list(\u0026quot;one\u0026quot;,\u0026quot;two\u0026quot;,c(\u0026quot;three\u0026quot;, \u0026quot;four\u0026quot;)) Y \u0026lt;- list(\u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;,c(\u0026quot;C\u0026quot;,\u0026quot;D\u0026quot;)) mapply(paste,X,Y) ## [[1]] ## [1] \u0026quot;one A\u0026quot; ## ## [[2]] ## [1] \u0026quot;two B\u0026quot; ## ## [[3]] ## [1] \u0026quot;three C\u0026quot; \u0026quot;four D\u0026quot; This is the same as doing:\nlist( paste(X[[1]],Y[[1]]), paste(X[[2]],Y[[2]]), paste(X[[3]],Y[[3]]) ) ## [[1]] ## [1] \u0026quot;one A\u0026quot; ## ## [[2]] ## [1] \u0026quot;two B\u0026quot; ## ## [[3]] ## [1] \u0026quot;three C\u0026quot; \u0026quot;four D\u0026quot; Here is one final example using rep(), which repeats the first argument a specific number of times\n X \u0026lt;- letters[1:4] Y \u0026lt;- 1:4 mapply(rep,X,Y) ## $a ## [1] \u0026quot;a\u0026quot; ## ## $b ## [1] \u0026quot;b\u0026quot; \u0026quot;b\u0026quot; ## ## $c ## [1] \u0026quot;c\u0026quot; \u0026quot;c\u0026quot; \u0026quot;c\u0026quot; ## ## $d ## [1] \u0026quot;d\u0026quot; \u0026quot;d\u0026quot; \u0026quot;d\u0026quot; \u0026quot;d\u0026quot;  ","date":1607212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607263508,"objectID":"581ce900716ce1deaf7ef26499e2852b","permalink":"https://michaelbarrowman.co.uk/post/the-lapply-family/","publishdate":"2020-12-06T00:00:00Z","relpermalink":"/post/the-lapply-family/","section":"post","summary":"This question on Reddit, got me thinking about the lapply() family of functions, and how a beginner might want to learn about them. Here is my take\nIntroduction The easiest one to understand is lapply(), I’ll work through that and then extend to the others.","tags":[],"title":"The lapply() family","type":"post"},{"authors":[],"categories":[],"content":"   Here, we’re going to take a quick look at the new pipe introduced in the development version of R 4.1.0, and compare it to the well-known %\u0026gt;% pipe from the {magrittr} package that is used throughout the {tidyverse}.\nThere was a recent update to {magrittr} which switched to implementing the bulk of the piping in the C language rather than directly in R. Because of this, as well as showing some features of the new base pipe, |\u0026gt;, I’m going to compare it to both the new {magrittr} pipe, %\u0026gt;% and the old version, which I am going to style as %\u0026gt;\u0026gt;%\ninstall.packages(\u0026quot;magrittr\u0026quot;) remotes::install_github(\u0026quot;Myko101/magrittrclassic\u0026quot;) If you want to install the classic {magrittr} without this updated %\u0026gt;\u0026gt;% pipe then run remotes::install_github(\"Myko101/magrittrclassic@classic\") to have it loaded as a package called {magrittrclassic} or remotes::install_github(\"tidyverse/magrittr@v1.5) to have it overwrite your current {magrittr} package. Note that this is prone to errors, particularly if {magrittr} or any packages that depend on it are loaded.\nThe first thing to inspect is the speed of this new pipe in a simple situation. Let’s create a simple function and see how it goes in the bench::mark() function\ndoubler \u0026lt;- function(val) 2*val x \u0026lt;- 1:10 bm \u0026lt;- bench::mark( standard = doubler(x), magrittrclassic = x %\u0026gt;\u0026gt;% doubler(), magrittr = x %\u0026gt;% doubler(), base = x |\u0026gt; doubler() ) ggplot2::autoplot(bm) Note that the `bench::mark()`` function by default also checks whether the results we get are the same.\nThe first thing that jumps out is just how slow the old {magrittr} implementation is and how fast the base/standard versions are. The time scale on the plot is logarithmic, which shows that the old {magrittr} function is almost 2 orders of magnitude slower (800ns vs 72.5 us), that’s nearly 100x slower!\nWhy is this? Firstly, the old {magrittr} pipe builds functions in R and then applies them to data turn by turn. However, the new {magrittr} pipe does all this in C. How is the base version so much faster? Well it is a syntax rather than an infix operator or a call.\nThis means that x %\u0026gt;% f() builds functions and performs actions to produce output which is identical to f(x). However, x |\u0026gt; f() is the same as f(x), it’s just a different way of writing it. Think of using a single quote, ' or a double quote \" to create a string, the command you’re giving to R is different, but the result is parsed identically before any actual R code is ran. Similarly, when you run 2 + 3 + 4, R will parse that as ( (2+3) + 4 )because the addition operator can only run on two objects so R has to divvy them up appropriately (left to right).\nThis can be evidenced by capturing the calls using the rlang::exprs() function\nrlang::exprs( standard = doubler(x), magrittrclassic = x %\u0026gt;\u0026gt;% doubler(), magrittr = x %\u0026gt;% doubler(), base = x |\u0026gt; doubler() ) ## $standard ## doubler(x) ## ## $magrittrclassic ## x %\u0026gt;\u0026gt;% doubler() ## ## $magrittr ## x %\u0026gt;% doubler() ## ## $base ## doubler(x) See the last one there? x |\u0026gt; doubler() is exactly doubler(x). There’s no transforming in R here, it just is the same thing.\nThis functionality is added to by the introduction of a new lambda function creation shortcut, let’s compare it to the {magrittr} implementation(s) of anonymous functions, using the dot notation:\nbm2 \u0026lt;- bench::mark( standard = (function(y) 2*y)(x), magrittrclassic = x %\u0026gt;\u0026gt;% {2*.}, magrittr = x %\u0026gt;% {2*.}, base = x |\u0026gt; \\(y) 2*y ) ggplot2::autoplot(bm2) Timings are very similar to the previous one, especially when only looking relatively. The slow down is probably due to the creation of a function in each use, which also explains why they are all around the same amount slower. What do these piped lambda functions look like?\nrlang::exprs( standard = (function(y) 2*y)(x), magrittrclassic = x %\u0026gt;\u0026gt;% {2*.}, magrittr = x %\u0026gt;% {2*.}, base = x |\u0026gt; \\(y) 2*y ) ## $standard ## (function(y) 2 * y)(x) ## ## $magrittrclassic ## x %\u0026gt;\u0026gt;% { ## 2 * . ## } ## ## $magrittr ## x %\u0026gt;% { ## 2 * . ## } ## ## $base ## (function(y) 2 * y)(x) Again the standard and base versions are parsed the same.\nOne final critic of the new pipe is that you can only pass an object to the first argument in a function. This is a limitation in a lot of cases, particularly because most {base} functions don’t follow the convention of passing the current data as the first argument. In {magrittr}, we can use a . to represent the piped data for other arguments, and if it appears at the top level (i.e. a direct argument) {magrittr} won’t also ut it i as the first argument. But using the lambda \\() syntax, we can get around this. We can also pass named arguments in the same way we usually would when calling a function. Let’s try it and time it\nmultiplier \u0026lt;- function(a,val) a*val bm3 \u0026lt;- bench::mark( standard = multiplier(2,x), magrittrclassic = x %\u0026gt;\u0026gt;% multiplier(2,.), magrittr = x %\u0026gt;% multiplier(2,.), base_named = x |\u0026gt; multiplier(a=2), base_lambda = x |\u0026gt; \\(y) multiplier(2,y) ) ggplot2::autoplot(bm3) Clearly, the lambda version of the base packages takes more time, again because it is creating the function in the middle, whereas the named version does not have to do this. Let’s capture them to check that this is true\nrlang::exprs( standard = multiplier(2,x), magrittrclassic = x %\u0026gt;\u0026gt;% multiplier(2,.), magrittr = x %\u0026gt;% multiplier(2,.), base_lambda = x |\u0026gt; \\(y) multiplier(2,y), base_named = x |\u0026gt; multiplier(a=2) ) ## $standard ## multiplier(2, x) ## ## $magrittrclassic ## x %\u0026gt;\u0026gt;% multiplier(2, .) ## ## $magrittr ## x %\u0026gt;% multiplier(2, .) ## ## $base_lambda ## (function(y) multiplier(2, y))(x) ## ## $base_named ## multiplier(x, a = 2) One final thing to look at is the lambda function part of this whole process. While the {tidyverse} doesn’t provide a general shortcut to produce these, they can be created within other functions. For example, the above syntax {2*.} only works within the context of a pipe and wouldn’t work as a piece of code on it’s own.\nThe other major way in which lambda functions are declared is through the {purrr} package. The {purrr} package provides methods of functional programming (to an extent), and so within a {purrr} function, we can define a function using the ~ symbol and, like the previous {tidyverse} lambda, using the . as the value being passed to the function. Let’s compare it to the \\() syntax, remember, this is again a syntax and not a function/call!\nlibrary(purrr,warn.conflicts=F) bm4 \u0026lt;- bench::mark( standard = { res \u0026lt;- vector(\u0026quot;list\u0026quot;,10) for(i in 1:10) res[[i]] \u0026lt;- mean(1:i) res }, purrr = map(1:10,~mean(1:.)), base = lapply(1:10,\\(i) mean(1:i)) ) ggplot2::autoplot(bm4) Again due to the lack of overheads for the \\() syntax, speed is definitely on it’s side. We could just as easily use the lapply() function here and declare the FUN argument using function(i) mean(1:i), but writing \\() is much quicker/easier.\nOne last thing to inspect is how these functions handle errors.\nthrow_error \u0026lt;- function(x){ stop(\u0026quot;OH NO!\u0026quot;) } Previously, the trace stack for {magrittr} was confusing and made it incredibly difficult to spot where the error came from. Let’s see how\n1:10 %\u0026gt;\u0026gt;% throw_error() ## Error in throw_error(.): OH NO! traceback() # 10: stop(\u0026quot;Why am I here?\u0026quot;) at #2 # 9: throw_error(.) # 8: function_list[[k]](value) # 7: withVisible(function_list[[k]](value)) # 6: freduce(value, `_function_list`) # 5: `_fseq`(`_lhs`) # 4: eval(quote(`_fseq`(`_lhs`)), env, env) # 3: eval(quote(`_fseq`(`_lhs`)), env, env) # 2: withVisible(eval(quote(`_fseq`(`_lhs`)), env, env)) # 1: 1:10 %\u0026gt;\u0026gt;% throw_error() Because of the structure of the old {magrittr}, numbers 2 - 8 are functions that are called internally within the pipe and so as end-users, they mean nothing!\nHowever, the new error handling, makes this much clearer without all the clutter:\n1:10 %\u0026gt;% throw_error() ## Error in throw_error(.): OH NO! traceback() # 3: stop(\u0026quot;Why am I here?\u0026quot;) at #2 # 2: throw_error(.) # 1: 1:10 %\u0026gt;% throw_error() Now let’s compare to the base pipe:\n1:10 |\u0026gt; throw_error() ## Error in throw_error(1:10): OH NO! traceback() # 2: stop(\u0026quot;Why am I here?\u0026quot;) at #2 # 1: throw_error(1:10) The trace is even shorter. This is because in the {magrittr} pipe, the actual pipe is considered to be a call, and so it appears first in the trace stack (bottom of the list), BUT the base pipe is not a call, and so it doesn’t appear there at all. Just like when capturing the expression, the values are already nested.\nUnlike errors though, warnings can be suppressed and code can continue, this means we can use the suppressWarnings() function to keep them quiet and just carry on. This is useful if you know about the warning beforehand, but is only recomended if you know exactly why the warning is appearing and just want your code to ignore it and run smoothly.\nthrow_warning \u0026lt;- function(x) { warning(\u0026quot;oh no\u0026quot;) x } This warning handling was one of the complaints about the old {magrittr} pipe,take the below which is instinctively what you would expect to do\n1:10 %\u0026gt;\u0026gt;% throw_warning() %\u0026gt;\u0026gt;% suppressWarnings() ## Warning in throw_warning(.): oh no ## [1] 1 2 3 4 5 6 7 8 9 10 It doesn’t work, instead you’d have to run\nsuppressWarnings( 1:10 %\u0026gt;\u0026gt;% throw_warning() ) ## [1] 1 2 3 4 5 6 7 8 9 10 Which does not look pleasant and means going back to the beginning of your pipeline if you get to the point of wanting to suppress warnings.\nThe new {magrittr} pipe and the {base} pipe don’t have such qualms and they are evaluated exactly as you would expect them to:\n1:10 %\u0026gt;% throw_warning() %\u0026gt;% suppressWarnings() ## [1] 1 2 3 4 5 6 7 8 9 10 1:10 |\u0026gt; throw_warning() |\u0026gt; suppressWarnings() ## [1] 1 2 3 4 5 6 7 8 9 10 ","date":1607212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607263508,"objectID":"f6c5fc86f60f43f538a4b9612edc8187","permalink":"https://michaelbarrowman.co.uk/post/the-new-base-pipe/","publishdate":"2020-12-06T00:00:00Z","relpermalink":"/post/the-new-base-pipe/","section":"post","summary":"Here, we’re going to take a quick look at the new pipe introduced in the development version of R 4.1.0, and compare it to the well-known %\u0026gt;% pipe from the {magrittr} package that is used throughout the {tidyverse}.","tags":[],"title":"The New Base Pipe","type":"post"},{"authors":[],"categories":[],"content":"  Introduction First, let’s load up what we need!\nset.seed(15102020) library(tidyverse) #We\u0026#39;ll use tidyverse functions library(magrittr) #A few extra pipes from magrittr library(lexicon) #For a word dictionary When dealing wth complex datasets, it is common that a variable may be stored as a character variable, when in reality what you want is a factor variable. On the surface, these two constructs look very similar:\neg_df \u0026lt;- tibble( c_var = c(\u0026quot;Cat\u0026quot;,\u0026quot;Dog\u0026quot;,\u0026quot;Cat\u0026quot;,\u0026quot;Mouse\u0026quot;,\u0026quot;Mouse\u0026quot;), f_var = factor(c(\u0026quot;Cat\u0026quot;,\u0026quot;Dog\u0026quot;,\u0026quot;Cat\u0026quot;,\u0026quot;Mouse\u0026quot;,\u0026quot;Mouse\u0026quot;)) ) eg_df ## # A tibble: 5 x 2 ## c_var f_var ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; ## 1 Cat Cat ## 2 Dog Dog ## 3 Cat Cat ## 4 Mouse Mouse ## 5 Mouse Mouse However, underneath they are treated quite differently. Behind the scenes, the factors are actually stored as integers with a special lookup table called their levels, which can be seen if we print the variable individually:\neg_df$f_var ## [1] Cat Dog Cat Mouse Mouse ## Levels: Cat Dog Mouse We can also see the hidden numbers by converting this to numeric:\nas.numeric(eg_df$f_var) ## [1] 1 2 1 3 3 The first element, Cat is associated with the first level, so it is stored as a 1, the third element is also Cat, so it is also stored as a 1. The fourth \u0026amp; fifth are both Mouse and so they’re stored as 3, indicating to use the third level.\n Why factors? Most statistical operations within R that can act on a character variable will essentially convert to a factor first. So, it’s more efficient to convert characters to factors before passing them into these kinds of functions. This also gives us more control over what we’re going to get.\nThis conversion makes many processes that work with characters a bit slow. If you’re wanting to do 20 functions on a data set and each one needs to convert your characters to factors internally before doing what it needs to, it’s clearly much faster to manually convert once before using these functions.\nFactors also take up slightly less space in your system’s memory. In R, this is approximately half the space of a character, however the way R stores this kind of data is surprisingly efficient. It’s definitely a good habit to get into if you ever want to move onto less efficient storage methods.\n Converting Above, I used the factor() function to quickly convert a single character variable to a factor variable. But what about if you’ve got a large dataset with many, many character variables that you want to convert to factors. What’s the smoothest way to do this?\nExample random dataset First, let’s create a large dataset, we’ll loop through a bunch of columns. We’ll use Fry’s 1000 Most Commonly Use English Words, as found in the sw_fry_1000 dataset from the {lexicon} package to choose random words for each variable. We’ll also throw in some numeric variables to make things harder:\ndf \u0026lt;- tibble(id=1:1000) #declare a tibble with just an id variable for(i in 1:10) { #How many distinct words should this variable have? distinct_words \u0026lt;- round(rexp(1,1/20)) +1 #What words can we choose from for this variable? these_words \u0026lt;- sample(sw_fry_1000,distinct_words) #What\u0026#39;s the name of this variable? this_name \u0026lt;- paste0(\u0026quot;var_\u0026quot;,ncol(df) + 1) #Generate the variable this_variable \u0026lt;- sample(these_words,1000,replace=T) #Store it in the tibble df[[this_name]] \u0026lt;- this_variable #Approximated 1/3 of the time, we\u0026#39;ll add a numeric variable if(rbinom(1,1,1/3) == 1){ this_name \u0026lt;- paste0(\u0026quot;var_\u0026quot;,ncol(df)+1) df[[this_name]] \u0026lt;- rnorm(1000) } } df ## # A tibble: 1,000 x 14 ## id var_2 var_3 var_4 var_5 var_6 var_7 var_8 var_9 var_10 var_11 var_12 ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 prop~ 1.83 row gove~ gene~ cow else women length 0.249 home ## 2 2 four -0.225 wind~ reas~ speak cow squa~ gold exerc~ 0.688 numer~ ## 3 3 leave 0.367 gold plant came cow egg human exerc~ -0.517 tell ## 4 4 rock 0.919 that meat gene~ cow leave human skill -0.280 fill ## 5 5 favor -1.01 mile nine tree cow very hand has -0.0302 left ## 6 6 shop 1.14 hunt drink speak cow take meat hit 0.908 over ## 7 7 end 0.0427 engi~ seas~ gene~ cow art women exerc~ 0.0395 unit ## 8 8 favor -0.647 body drink gene~ cow diff~ doll~ most -0.458 people ## 9 9 earth -2.47 fight nine tree cow deci~ air king 0.0182 child ## 10 10 end 1.35 prot~ drink speak cow carry women grand -0.978 conti~ ## # ... with 990 more rows, and 2 more variables: var_13 \u0026lt;chr\u0026gt;, var_14 \u0026lt;dbl\u0026gt; The generation of this data is actually rather clunky as it’s using a loop, and we’re going to avoid that. Instead, we’re going to turn all these characters into factors in a single line. Here’s the line of code which will update the dataset, followed by the explanation:\n  The solution With {tidyverse} processes, the key thing we’re trying to do is build a “sentence” explaining what we’re doing. Here’s our expression, followed by the English sentence equivalent\ndf %\u0026lt;\u0026gt;% mutate(across(where(is.character),as_factor)) #Update the df by mutating it across variables where it is a # character by performing as_factor on them df ## # A tibble: 1,000 x 14 ## id var_2 var_3 var_4 var_5 var_6 var_7 var_8 var_9 var_10 var_11 var_12 ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 1 prop~ 1.83 row gove~ gene~ cow else women length 0.249 home ## 2 2 four -0.225 wind~ reas~ speak cow squa~ gold exerc~ 0.688 numer~ ## 3 3 leave 0.367 gold plant came cow egg human exerc~ -0.517 tell ## 4 4 rock 0.919 that meat gene~ cow leave human skill -0.280 fill ## 5 5 favor -1.01 mile nine tree cow very hand has -0.0302 left ## 6 6 shop 1.14 hunt drink speak cow take meat hit 0.908 over ## 7 7 end 0.0427 engi~ seas~ gene~ cow art women exerc~ 0.0395 unit ## 8 8 favor -0.647 body drink gene~ cow diff~ doll~ most -0.458 people ## 9 9 earth -2.47 fight nine tree cow deci~ air king 0.0182 child ## 10 10 end 1.35 prot~ drink speak cow carry women grand -0.978 conti~ ## # ... with 990 more rows, and 2 more variables: var_13 \u0026lt;fct\u0026gt;, var_14 \u0026lt;dbl\u0026gt; And as if by magic, all of the characters are now factors (note the \u0026lt;fct\u0026gt; under the variable names).\n The Explanation The above code uses five functions, and an operation to perform the action. We’ll dig down into the functions and then climb back out as their results are processed:\n %\u0026lt;\u0026gt;% grabs the tibble on it’s left hand side and passes it to the function on the right. At this point, it works exactly like the regular %\u0026gt;% operator  mutate() means we are creating or updating a variable inside the tibble  across() allows us perform a function across many variables within the tibble  where() allows us to specify where we want across() to perform the function  is.character(), in the above line, we don’t use the brackets for is.character() because we’re not applying it, we’re referencing it. We’re telling the where() function to use this when checking where we want the function to be applied. The is.character() function returned TRUE when the variable is a character and FALSE when it isn’t (e.g. a numeric)  where() therefore applies this function to every variable in df and returns a vector of TRUE and FALSE to across() to indicate which variables in the tibble we want across() to act on as_factor() converts things (e.g. characters) into factors.  across() has now been passed a logical vector telling it which columns to apply a function and a function that it needs to apply. So it does just that and outputs another tibble  mutate() has then been passed a tibble for it’s first argument (df via the %\u0026lt;\u0026gt;% pipe) and another tibble as the output of across(). It stitches these together, if there are any names in common, it overwrites those in df with those from across(). All the variables in across() will also appear in df because that’s where they came from, so the old values are overwritten with the new ones  %\u0026lt;\u0026gt;% then receives this new tibble from mutate() and stores it back into the df tibble that we originally passed to it. This is essentially saying that df %\u0026lt;\u0026gt;% f() is the same as df \u0026lt;- df %\u0026gt;% f(), that’s why this is called the assignment pipe or updating pipe.   ","date":160272e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602770708,"objectID":"7f9169d35291ee59820987aa3e355f5b","permalink":"https://michaelbarrowman.co.uk/post/convert-all-character-variables-to-factors/","publishdate":"2020-10-15T00:00:00Z","relpermalink":"/post/convert-all-character-variables-to-factors/","section":"post","summary":"Introduction First, let’s load up what we need!\nset.seed(15102020) library(tidyverse) #We\u0026#39;ll use tidyverse functions library(magrittr) #A few extra pipes from magrittr library(lexicon) #For a word dictionary When dealing wth complex datasets, it is common that a variable may be stored as a character variable, when in reality what you want is a factor variable.","tags":[],"title":"Convert all Character variables to Factors","type":"post"},{"authors":["Glen Philip Martin","David A Jenkins","Lucy Bull","Rose Sisk","Lijing Lin","William Hulme","Anthony Wilson","Wenjuan Wang","Dr. Michael Barrowman, PhD","Camilla Sammut-Powell","Alexander Pate","Matthew Sperrin","Niels Peek","Predictive Healthcare Analtics Group"],"categories":null,"content":"","date":1595721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595721600,"objectID":"07813a380f844215dd90c395b722189b","permalink":"https://michaelbarrowman.co.uk/publication/martin-2020a/","publishdate":"2020-07-26T00:00:00Z","relpermalink":"/publication/martin-2020a/","section":"publication","summary":"__Background and Objective__\n\nIn view of the growth of published articles, there is an increasing need for studies that summarize scientific research. An increasingly common review is a \"methodology scoping review,\" which provides a summary of existing analytical methods, techniques and software that have been proposed or applied in research articles to address an analytical problem or further an analytical approach. However, guidelines for their design, implementation, and reporting are limited.\n\n__Methods__\n\nDrawing on the experiences of the authors, which were consolidated through a series of face-to-face workshops, we summarize the challenges inherent in conducting a methodology scoping review and offer suggestions of best practice to promote future guideline development.\n\n__Results__\n\nWe identified three challenges of conducting a methodology scoping review. First, identification of search terms; one cannot usually define the search terms a priori, and the language used for a particular method can vary across the literature. Second, the scope of the review requires careful consideration because new methodology is often not described (in full) within abstracts. Third, many new methods are motivated by a specific clinical question, where the methodology may only be documented in supplementary materials. We formulated several recommendations that build upon existing review guidelines. These recommendations ranged from an iterative approach to defining search terms through to screening and data extraction processes.\n\n__Conclusion__\n\nAlthough methodology scoping reviews are an important aspect of research, there is currently a lack of guidelines to standardize their design, implementation, and reporting. We recommend a wider discussion on this topic.","tags":null,"title":"Toward a Framework for the Design, Implementation, and Reporting of Methodology Scoping Reviews","type":"publication"},{"authors":[],"categories":[],"content":"  There is no greater staple of the {tidyverse} than the pipe, %\u0026gt;%, however not a lot of people understand what’s going on “under-the-bonnet” of the pipe. To be fair, not many people have to worry about it. Until you start trying to do a bit of meta-programming. Then things can get difficult.\nRecently, a question posed by user preposterior on RStudio Community embodied an issue that can happen when trying to extract the name of a variable.\nWhen using the {rlang} package, we can get the name of a variable passed into a function using the following:\nsimple_get_name \u0026lt;- function(x){ x_sym \u0026lt;- ensym(x) as_name(x_sym) } my_variable \u0026lt;- 1 simple_get_name(my_variable) ## [1] \u0026quot;my_variable\u0026quot; However, when we try to use this with a pipe, it goes wrong:\nmy_variable %\u0026gt;% simple_get_name() ## [1] \u0026quot;.\u0026quot; What is this!? Where did that dot come from? Well, that’s why you’re here. The . can actually be used as a variable name in R, it’s perfectly syntactic, although ill-advised. Many functions (particularly {tidyverse} ones) use the . as a filler for other purposes, the pipe is a big example, but it is also prominent in the map() family of functions in {purrr}.\nThe reason we get this output is because the pipe actually turns your pipeline of functions into a chain of new functions defined something like this:\nfunction(.) simple_get_name(.) So, it’s a chain of wrapper functions around your pipeline’d functions. The functions are direct copies of what you use in your pipeline. For example, if you have a few arguments, the . will be inserted as the first argument. This makes sense as this is what the pipe does (passes your input into the first agrument, unless told otherwise).\nOnce the pipe has this chain of functions, it uses the freduce() function to apply the functions in order to the output of the previous one. You already knew what a pipeline did, know you’ve got a little insight into how.\nSo how do we pull out that my_variable name from within a piped function? Well the problem is that, within the context of that function, that variable is lost. It’s value has been put into the variable ., but that original name is long gone.\nWe can, however, look back over the call-stack where the current function is being evaluated (which is what error-finding functions like traceback() do). Within the pipe, it actually creates a relatively deeply nested set of calls (about 9 calls deep). However, the sys.calls() function can return this stack. Compare for example the following two outputs:\nstack_fun \u0026lt;- function(x){ sys.calls() } stack_fun(my_variable) my_variable %\u0026gt;% stack_fun ## [[1]] ## stack_fun(my_variable) ## [[1]] ## my_variable %\u0026gt;% stack_fun ## ## [[2]] ## withVisible(eval(quote(`_fseq`(`_lhs`)), env, env)) ## ## [[3]] ## eval(quote(`_fseq`(`_lhs`)), env, env) ## ## [[4]] ## eval(quote(`_fseq`(`_lhs`)), env, env) ## ## [[5]] ## `_fseq`(`_lhs`) ## ## [[6]] ## freduce(value, `_function_list`) ## ## [[7]] ## withVisible(function_list[[k]](value)) ## ## [[8]] ## function_list[[k]](value) ## ## [[9]] ## stack_fun(.) The first element of this stack will be the initial call, in this case my_variable %\u0026gt;% stack_fun(). This will be a call object and so we can pull out the left-hand side by extracting the second element (the %\u0026gt;% is the first element, and stack_fun is the third). Therefore, the previous function can be written as:\nstacked_get_name \u0026lt;- function(x){ first_call \u0026lt;- sys.calls()[[1]] #get the first entry on the call stack lhs \u0026lt;- first_call[[2]] #get the second element of this entry z \u0026lt;- rlang::as_name(lhs) print(z) } my_variable %\u0026gt;% stacked_get_name() ## [1] \u0026quot;my_variable\u0026quot; It worked! Brilliant!\nBut, that’s not the end of our tale!\nThis is just looking for the initial call, and isn’t strictly going to seek out where there is a pipe. For example, it wouldn’t work with the following function, since wrap_stacked_get_name() would be at the top of the stack:\nwrap_stacked_get_name \u0026lt;- function(wrapped_var){ this_variable \u0026lt;- wrapped_var+1 this_variable %\u0026gt;% stacked_get_name } wrap_stacked_get_name(my_variable) ## [1] \u0026quot;my_variable\u0026quot; This should return this_variable, but since it’s looking at the initial call, it looks too far back up the call-stack and misses this variable.\nHowever, by inspecting the entire stack for a pipe, we can pull out the most recent (i.e. the lowest) entry that is a pipe, and grab the left-hand side of that call.\nget_lhs \u0026lt;- function(){ calls \u0026lt;- sys.calls() #pull out the function or operator (e.g. the `%\u0026gt;%`) call_firsts \u0026lt;- lapply(calls,`[[`,1) #check which ones are equal to the pipe pipe_calls \u0026lt;- vapply(call_firsts,identical,logical(1),quote(`%\u0026gt;%`)) #if we have no pipes, then get_lhs() was called incorrectly if(all(!pipe_calls)){ NULL } else { #Get the most recent pipe, lowest on the pipe_calls \u0026lt;- which(pipe_calls) pipe_calls \u0026lt;- pipe_calls[length(pipe_calls)] #Get the second element of the pipe call this_call \u0026lt;- calls[[c(pipe_calls,2)]] #We need to dig down into the call to find the original while(is.call(this_call) \u0026amp;\u0026amp; identical(this_call[[1]],quote(`%\u0026gt;%`))){ this_call \u0026lt;- this_call[[2]] } this_call } } Once we have the call, getting the lhs of it requires digging down. If we have pipeline, then it’s actually a nested sequence of operators. For example, 2+3+4 makes sense to us, but R can’t add like this, it breaks this down by calculating from left to right, basically it does this (2 + 3) + 4, which is the same as add(add(2,3),4). R does this with the pipe too.\nIf we’re piping a few things together, we write this: my_variable %\u0026gt;% fun1 %\u0026gt;% fun2 %\u0026gt;% fun3, R reads it as this: ((my_variable %\u0026gt;% fun1) %\u0026gt;% fun2) %\u0026gt;% fun3.\nSo we repeatedly check that the current function/operator/call name is a pipe, if it is, grab the second entry (which is what is being piped into the current pipe). If it isn’t, we’ve dug down far enough.\nSo, now that we have that little function, we can re-write our function to check for this first:\nget_name \u0026lt;- function(x){ lhs \u0026lt;- get_lhs() if(is.null(lhs)){ lhs \u0026lt;- rlang::ensym(x) } as_name(lhs) } get_name(my_variable) ## [1] \u0026quot;my_variable\u0026quot; my_variable %\u0026gt;% get_name ## [1] \u0026quot;my_variable\u0026quot; Eureka! Now, let’s check the wrapper function:\nwrap_get_name \u0026lt;- function(wrapped_var){ this_variable \u0026lt;- wrapped_var+1 this_variable %\u0026gt;% get_name } wrap_get_name(my_variable) ## [1] \u0026quot;this_variable\u0026quot; my_variable %\u0026gt;% wrap_get_name ## [1] \u0026quot;this_variable\u0026quot; This function acts a little strange around fseq functions. But, the results make sense when you think about it.\nfseq_get_name \u0026lt;- . %\u0026gt;% get_name This method of creating a function, where the initial starting value of the pipeline is actually that previously discussed ., this is essentially the same as the previous, wrap_get_name() function:\nfseq_get_name_dummy \u0026lt;- function(.){ . %\u0026gt;% get_name } As usual, we can use this function in one of two ways, either as a regular function with brackets or as a piped function\nfseq_get_name(my_variable) ## [1] \u0026quot;.\u0026quot; Looking at the alternate definition above, this makes sense as a result. The pipeline starts with a .\nmy_variable %\u0026gt;% fseq_get_name ## [1] \u0026quot;my_variable\u0026quot; What? This time, it’s returned the value being piped in. But, if we imagine a fseq-style as sticking the pipelines together, then the actual start of this pipeline is the my_variable.\nAnd there we go. You’ve now got a bit more of an understanding of the sys.calls() function and can extract the name of a variable being passed into a pipeline. This is a very basic way of doing it, it doesn’t do nearly enough checks as a function in-production would have to do, but it’s a good start. You could also extract any part of that original pipeline call.\nSidenote for the pro’s out there. This page is written in {rmarkdown} and rendered using {blogdown}. This meant that when I used the sys.calls() function, I actually got a much deeper nesting of calls when rendering in these than in my RStudio application. This is because when rendering, each code chunk is evaluated within another call. For the local render with {rmarkdown}, I had to remove the first 18 calls before the “first” call was the one actually used above. For the {blogdown} render, it is 24. This page has a bunch of hidden code chunks (using eval=F and echo=F) to make the code and the output look seamless. ","date":1595548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595546735,"objectID":"a8671e08d68476048a6b1217403caab1","permalink":"https://michaelbarrowman.co.uk/post/getting-a-variable-name-in-a-pipeline/","publishdate":"2020-07-24T00:00:00Z","relpermalink":"/post/getting-a-variable-name-in-a-pipeline/","section":"post","summary":"There is no greater staple of the {tidyverse} than the pipe, %\u0026gt;%, however not a lot of people understand what’s going on “under-the-bonnet” of the pipe. To be fair, not many people have to worry about it.","tags":[],"title":"Getting a variable name in a pipeline","type":"post"},{"authors":[],"categories":[],"content":"Within git, the default branch is usually named master, however in recent times, the negative connotations of that word are coming to the forefront of a lot of people\u0026rsquo;s minds, and so they are wishing to diverge away from that kind of terminology. The simplest change that we can make is to default to the HEAD branch of a repo, which will point towards whatever the actual default branch is for a the repo, whether that is master, main or Captain.\nUnfortunately, this change can be slow, and although resources like GitHub have expressed interest in switching away from the default master, some things are still hardcoded. One of which is the limitations of GitHub Pages deployment. Users can currently choose from one of three options:\n Build website from the master branch Build website from the docs folder in the master branch Build website from the gh-pages branch  The use of master here is hardcoded, and many users currently choose to use the docs folder in the master branch as the location to store their website. Depending on workflows, the other two options might not be possible, or would require huge restructuring of your workflow if you wish to switch from master. For User pages (repos that are \u0026lt;username\u0026gt;.github.io), they can only be built from the master branch; hopefully this will change soon (see here).\nFor example, this blog is written using blogdown, a package for R, which helps to create blogs. In doing so, it creates a static site in a subdirectory called public (by default). You do your work in the parent folder, it generates content in this subfolder. If you wish to use GitHub Pages to publish your site, the recommendations by yihui, the package autho, are to set up your git directory inside this subdirectory. This has the limitation of meaning the content of your parent folder is not backed up to GitHub, and is only stored locally.\nHowever, thanks to GitHub user s0 and their GitHub Action, it\u0026rsquo;s possible to keep your work inside the doc folder on your default branch (whatever it\u0026rsquo;s name may be) and have that folder automatically pushed to the gh-pages branch.\nFor those who don\u0026rsquo;t know, GitHub Actions allows automation when certain events (triggers) occur within your repo. You can try to write your own complicated commands, or use those created by other users within a relatively simple skeleton. We\u0026rsquo;re going to use one of these simple skeletons to utilise s0\u0026rsquo;s work, thus allowing us to use the main branch, rather than the master branch on our repo. In order to use GitHub Actions, we need to add a .yaml file (stands for YAML Ain\u0026rsquo;t Markup Language) within our repo.\nRename the branch The first step in this process would be to actually change the name of our master branch to main (or whatever you choose).\nDirectly on GitHub GitHub doesn\u0026rsquo;t directly support renaming of branches (as far as I know). So, what we need to do is to create a new branch for our repository by clicking on the branches button at the top left of our repo Code page (probably says master right now).\nAnd then type in the name of the new branch (e.g. main). If this branch doesn\u0026rsquo;t exist, you\u0026rsquo;ll be given the option to Create branch: main from 'master'. Click on Settings then Branches and you can change your default branch to the new main branch.\nYou can then delete your master branch, although it might be worth holding on to it for a little while, in case anybody downstream from you is using it, or as a back-up in case something goes wrong!\nGit Bash If you have a local copy of your repo, you can run the following in command line to rename it:\ngit branch -m master main  If you want to rename the current branch, you can simplify this to be:\ngit branch -m main  Note you will have to use -M instead of -m if you are renaming a branch and only changing captalisation, e.g. from main to Main.\nA common error when running this command is the following (or something to this effect):\nerror: refname refs/heads/HEAD not found fatal: Branch rename failed  This means you don\u0026rsquo;t have a branch checked out, and so you\u0026rsquo;ll have to create a new branch, but when doing so, you can name it whatever you want\ngit checkout -b main  After you\u0026rsquo;ve done this locally, you\u0026rsquo;ll have to git push your repo up to GitHub again. However, you\u0026rsquo;ll probably get an error telling you to run the following instead\ngit push --set-upstream origin main  This will just ensure your new main branch is upstream of the previous master branch.\nThen you\u0026rsquo;ll have to change GitHub\u0026rsquo;s default branch to your new one in the Settings as above.\nGitHub Workflows GitHub Action files are stored in a special directory in your repo, the .github/workflow directory. All we have to do is create a file in this directory, name it something useful and give it the .yaml extension. Sounds simple, and for most people it is. The only limitation is that sometimes, we can\u0026rsquo;t create folders with the . at the start (particularly on Windows). Or at least, we can\u0026rsquo;t create them in the usual Right Click \u0026gt; New \u0026gt; Folder method in Windows Explorer. The simple way is to use Command Line to do it.\nmkdir .github  The mkdir command makes directories. It\u0026rsquo;s as simple as that.\nAlternatively, you can create this directly on GitHub in the usual manner. Just remember that you will have to git pull any changes you make this way.\nThe YAML The YAML file that we create will look like this:\nname: 'Deploy to gh-pages' on: push:\tbranches: - main paths:\t- 'docs/**' jobs: deploy: name: Push docs to gh-pages runs-on: ubuntu-latest steps: - uses: actions/checkout@main - name: Deploy uses: s0/git-publish-subdir-action@develop env: REPO: self BRANCH: gh-pages FOLDER: docs GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}  Feel free to copy the above directly. If you\u0026rsquo;re using RStudio to manage your repo, you can create a New Text File, save it with the extension .yaml and RStudio will conveniently colour code the file. Otherwise, you can do it in notepad (just make sure the extension sticks).\nWhat does this file do? I don\u0026rsquo;t think it\u0026rsquo;s too important to go into great details here, there is plenty of reading to be done on GitHub Actions. So, just a quick look at the important bits, in case you want to change something. Apart from the name: line, it\u0026rsquo;s split into two parts, the on: and the jobs: parts. Note that in YAML files, whitespace is important, and gives structure: push: is a child of on: and jobs: is the parent of deploy:, but on: and jobs: are siblings.\nThe Triggers The on: part contains information on what events will trigger the event. Here, we\u0026rsquo;re telling GitHub that we want this Action to run when we push our repo to the main branch, only if something has changed in the docs folder (or path). This means that any pushes that happen to other branches will be ignored, and any pushes that don\u0026rsquo;t change our docs folder will also be ignored. The syntax for paths: actually allows you to check for changes to anything that matches this string, so by using 'docs/**', we match anything that starts with 'docs/', i.e. anything within the docs folder. This is useful because we\u0026rsquo;re building our gh-pages branch based solely on what\u0026rsquo;s in docs. If something changes elsewhere in the repo, it doesn\u0026rsquo;t matter (even if you are accessing data in your repo, but outside of your site because those changes will still be pushed to your repo, just don\u0026rsquo;t need to trigger a site rebuild). It also doesn\u0026rsquo;t matter what happens on other branches (such as a development branch) because we\u0026rsquo;re not wanting to build our GitHub pages from them.\nThe Actions The jobs: part contains the actual actions that occurs. You can have many Actions and jobs within he same file, but here we only have one job, which consists of two tasks. deploy: is just the formal name for the job that we\u0026rsquo;re running. If we want more jobs to run, we can give them different names and place them at the same hierarchical starting point as deploy: (i.e. with two spaces in front). Different jobs will run in parallel, each individual job will run in order.\nWe then give a bit of information about the job, first it\u0026rsquo;s name Push docs to gh-pages (more like a title), followed by what operating system we want GitHub Actions to use to implement it. Finally, we have the steps:, which is where we put the list of tasks that need to be run (in order).\nThis Action only has two steps and they are both uses: steps, which basically means we\u0026rsquo;re going to be using Actions that are properly defined elsewhere. We could write an action directly here in quotes and supply the name of what application we want it to be run in (at a Command Line level), but we don\u0026rsquo;t have to since these Actions are defined for us. Each task starts with a - followed by the information for that task.\nCheckout The first task is simple - uses: actions/checkout@master. You may recognise the format of this as it comes up a lot within GitHub, it is \u0026lt;user\u0026gt;/\u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt;. This is because all Actions created by other users, are actually repos. So what we\u0026rsquo;re doing here is saying we want to use the Action defined within the main branch of the checkout repo made by the user actions and we can actually view that repo here, or since it is an Action, we can view it on the Actions Marketplace here.\nThis Action essentially runs a git checkout command on your repo so that it\u0026rsquo;s files can be accessed by your workflow. Actions that change your repo in some way will typically start with this. They will usually also end with something that commits and pushes the results back onto your repo. We don\u0026rsquo;t need to do this part because it is covered by the second task\nDeploy Task number two is where the magic happens. There\u0026rsquo;s a lot more here than in the first task\n- name: Deploy uses: s0/git-publish-subdir-action@develop env: REPO: self BRANCH: gh-pages FOLDER: docs GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}  We have three children within this task, name:, uses: and env: and env: even has some children of it\u0026rsquo;s own. Firstly, we\u0026rsquo;re giving this task a name, Deploy; this isn\u0026rsquo;t necessary, but it looks a little neater and makes it clearer what this task is doing (useful if you\u0026rsquo;re running a lot of tasks in a single job).\nThe next child is the same as previously, uses: s0/git-publish-subdir-action@develop. We\u0026rsquo;re going to run the action on the develop branch of the git-publish-subdir-action repo by the user s0. Once again, you can view this repo here or as an Action on the Marketplace here. This is main part of what we\u0026rsquo;re doing. This Action does the actual copying of the subdirectory and pushes it to a new branch.\nThe last child is env: and this is where you might have to change things depending on your use-case. This has four children, which are actually variables. Just like in most programming, we work within an environment that contains variables, well here we\u0026rsquo;re going to define some for the git-publish-subdir-action to use.\nYou don\u0026rsquo;t need to worry about REPO and GITHUB_TOKEN, these just mean that the action is going to run on the current repo (REPO: self) and provides authentication that it\u0026rsquo;s really us doing the changes (by generating a GITHUB_TOKEN to use as an auth token). The other two variables are important, it\u0026rsquo;s telling the Action what directory we want to copy, which by default (if you\u0026rsquo;re been running your GitHub Pages using the master/docs form) is currently set to be docs, but this can be any other folder (or sub-folder) in your repo, e.g public/home, my-gh-pages-site or \u0026quot;My Homepage\u0026quot; (don\u0026rsquo;t forget the quotes). Then finally, the name of the branch we want to put it on. If you\u0026rsquo;re looking here with the intention of using a GitHub Page, then this will have to be gh-pages (unless using a User account), but can be any name you want your new branch to be.\nCheck it works Finally, once we\u0026rsquo;ve done all this, we can git push to the main branch of our repo on GitHub and it should build our website (provided we have the GitHub pages set to use gh-pages). To check whether this has worked, simply load up your GitHub Page. You can also have a look at the run through of the Action in the Actions tab in your repo home. This gives you the output of the Command Line of every step of your Action (although remember there are only two).\nUser repos User repos are named like \u0026lt;username\u0026gt;.github.io and can only be built from the master branch. This is because User repos are expected to be self-contained and their GitHub Pages site can only be built from the master branch. This means that you are expected to have an index.html file in your home directory. However, this can limit the ways in which your site can be built. To fix this, we can use the workflow as above, but instead of pushing the subdirectory to the gh-pages branch, we push it to the master branch, whilst still using our main branch as our default. The only change we need to do, is to replace the BRANCH: gh-pages line in the YAML file with BRANCH: master (make sure you keep the whitespace before it).\nIf you are using a Custom Domain, there is one last thing that you\u0026rsquo;ll need to consider. When you add a Custom Domain, GitHub stores this as the CNAME file in your home directory. The Action above destroys the master branch before rebuilding it from scratch. This includes deleting that CNAME file and, since it isn\u0026rsquo;t in the subdirectory of your main branch, it doesn\u0026rsquo;t get put back in. The solution? Just put it in there yourself. This simply means adding a file to your docs directory called CNAME (no file extension) and have it\u0026rsquo;s only contents be your url. Since new files without extensions can, again, be tricky. At Command Line:\necho -n [YOUR_URL_HERE] \u0026gt; docs/CNAME  (The use of -n here means that a newline isn\u0026rsquo;t added to the end of the line)\n","date":1595376e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595412242,"objectID":"480d77140dd6b32aecb0d34fa37c699e","permalink":"https://michaelbarrowman.co.uk/post/deploy-to-github-pages/","publishdate":"2020-07-22T00:00:00Z","relpermalink":"/post/deploy-to-github-pages/","section":"post","summary":"Within git, the default branch is usually named master, however in recent times, the negative connotations of that word are coming to the forefront of a lot of people\u0026rsquo;s minds, and so they are wishing to diverge away from that kind of terminology.","tags":[],"title":"Deploy to GitHub Pages","type":"post"},{"authors":["Dr. Michael Barrowman, PhD","Niels Peek","Mark Lambie","Glen Philip Martin","Matthew Sperrin"],"categories":null,"content":"","date":1564531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564531200,"objectID":"17682ac75413d2cd4d1e1893c411e00d","permalink":"https://michaelbarrowman.co.uk/publication/barrowman-2019a/","publishdate":"2019-07-31T00:00:00Z","relpermalink":"/publication/barrowman-2019a/","section":"publication","summary":"__Background__\n\nAnalysis of competing risks is commonly achieved through a cause specific or a subdistribution framework using Cox or Fine \u0026 Gray models, respectively. The estimation of treatment effects in observational data is prone to unmeasured confounding which causes bias. There has been limited research into such biases in a competing risks framework.\n\n__Methods__\n\nWe designed simulations to examine bias in the estimated treatment effect under Cox and Fine \u0026 Gray models with unmeasured confounding present. We varied the strength of the unmeasured confounding (i.e. the unmeasured variable's effect on the probability of treatment and both outcome events) in different scenarios.\n\n__Results__\n\nIn both the Cox and Fine \u0026 Gray models, correlation between the unmeasured confounder and the probability of treatment created biases in the same direction (upward/downward) as the effect of the unmeasured confounder on the event-of-interest. The association between correlation and bias is reversed if the unmeasured confounder affects the competing event. These effects are reversed for the bias on the treatment effect of the competing event and are amplified when there are uneven treatment arms.\n\n__Conclusion__\n\nThe effect of unmeasured confounding on an event-of-interest or a competing event should not be overlooked in observational studies as strong correlations can lead to bias in treatment effect estimates and therefore cause inaccurate results to lead to false conclusions. This is true for cause specific perspective, but moreso for a subdistribution perspective. This can have ramifications if real-world treatment decisions rely on conclusions from these biased results. Graphical visualisation to aid in understanding the systems involved and potential confounders/events leading to sensitivity analyses that assumes unmeasured confounders exists should be performed to assess the robustness of results.","tags":null,"title":"How Unmeasured Confounding in a Competing Risks Setting Can Affect Treatment Effect Estimates in Observational Studies","type":"publication"},{"authors":["Alexander Pate","Dr. Michael Barrowman, PhD","David Webb","Jeanne M Pimenta","Kourtney J Davies","Rachael Williams","Tjeerd Van Staa","Matthew Sperrin"],"categories":null,"content":"","date":1540425600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540425600,"objectID":"2837f6b65285ddb7cd8bbc2884ea9347","permalink":"https://michaelbarrowman.co.uk/publication/pate-2018a/","publishdate":"2018-10-25T00:00:00Z","relpermalink":"/publication/pate-2018a/","section":"publication","summary":"__Introduction__\n\nTraditional phase IIIb randomised trials may not reflect routine clinical practice. The Salford Lung Study in chronic obstructive pulmonary disease (SLS COPD) allowed broad inclusion criteria and followed patients in routine practice. We assessed whether SLS COPD approximated the England COPD population and evidence for a Hawthorne effect.\n\n__Methods__\n\nThis observational cohort study compared patients with COPD in the usual care arm of SLS COPD (2012-2014) with matched non-trial patients with COPD in England from the Clinical Practice Research Datalink database. Generalisability was explored with baseline demographics, clinical and treatment variables; outcomes included COPD exacerbations in adjusted models and pretrial versus peritrial comparisons.\n\n__Results__\n\nTrial participants were younger (mean, 66.7 vs 71.1 years), more deprived (most deprived quintile, 51.5% vs 21.4%), more current smokers (47.5% vs 32.1%), with more severe Global initiative for chronic Obstructive Lung Disease stages but less comorbidity than non-trial patients. There were no material differences in other characteristics. Acute COPD exacerbation rates were high in the trial population (98.37th percentile).\n\n__Conclusion__\n\nThe trial population was similar to the non-trial COPD population. We observed some evidence of a Hawthorne effect, with more exacerbations recorded in trial patients; however, the largest effect was observed through behavioural changes in patients and general practitioner coding practices.","tags":null,"title":"Study Investigating the Generalisability of a COPD Trial Based in Primary Care (Salford Lung Study) and the Presence of a Hawthorne Effect","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c9b5771543b03b8149b612b630936a56","permalink":"https://michaelbarrowman.co.uk/experience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/experience/","section":"","summary":"","tags":null,"title":"Experience","type":"widget_page"}]