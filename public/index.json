[{"authors":["admin"],"categories":null,"content":"Michael Barrowman is currently finalising his Thesis on Multi-State Clinical Prediction Models in Renal Replacement Therapy as a PhD Candidate within the University of Manchester, whilst tutoring Maths, Stats \u0026amp; IT at Liverpool John Moores University.\nHis PhD project encompasses the development and validation of a multi-state clinical predication model, as well as the methodological advancements to produce such a model. This has led to multiple publications and the creation of software as a by-product.\nHe has previously worked within both the public and private sector providing data analysis to many industries, particularly education and health. During this time, he has contributed to SAPs and SOPs for a pioneering pragmatic clinical trial and improved the efficiency of examination marking by over 10%.\nHe is interested in Data Science, particularly using R and RStudio to their fullest potential, encouraging others to do the same and is an advocate for neat and reproducible coding practices.\nHe lives in Merseyside, UK and he enjoy walks down by the local canal and to the park with his two children, visiting castles \u0026amp; historic monuments and camping.\n","date":1595721600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1595721600,"objectID":"8c1425bb4f5a5b836c1d93a1a8e2f00e","permalink":"https://michaelbarrowman.co.uk/author/michael-barrowman/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/michael-barrowman/","section":"authors","summary":"Michael Barrowman is currently finalising his Thesis on Multi-State Clinical Prediction Models in Renal Replacement Therapy as a PhD Candidate within the University of Manchester, whilst tutoring Maths, Stats \u0026amp; IT at Liverpool John Moores University.","tags":null,"title":"Michael Barrowman","type":"authors"},{"authors":null,"categories":null,"content":"","date":1592784000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592784000,"objectID":"2794fcc40e76c39c475a81d1ff126c38","permalink":"https://michaelbarrowman.co.uk/package/mpipe/","publishdate":"2020-06-22T00:00:00Z","relpermalink":"/package/mpipe/","section":"package","summary":"The mpipe package is designed to add extra functionality to the pipeline process in tidyverse style R usage","tags":["R Package"],"title":"mpipe","type":"package"},{"authors":null,"categories":null,"content":"","date":1595289600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595289600,"objectID":"3c651fb0165c246151d4f319ece9b8e6","permalink":"https://michaelbarrowman.co.uk/package/mutils/","publishdate":"2020-07-21T00:00:00Z","relpermalink":"/package/mutils/","section":"package","summary":"The goal of mutils is to provide useful functions to make data processing smoother. Most functions contained here are 'nifty', rather than 'innovative'.","tags":["R Package"],"title":"mutils","type":"package"},{"authors":null,"categories":null,"content":"","date":1592092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592092800,"objectID":"b9e4cd6f5a58b4a8277e294b4b3e1aab","permalink":"https://michaelbarrowman.co.uk/package/typos/","publishdate":"2020-06-14T00:00:00Z","relpermalink":"/package/typos/","section":"package","summary":"The goal of typos is to provide a flexible warning when commonly mis-typed functions are called. Functions with typing errors will still be evaluated and a warning will be output. It also provides the user with a convenient function to define their own typos.","tags":["R Package"],"title":"typos","type":"package"},{"authors":[],"categories":[],"content":"\r\r\rHere, we’re going to take a quick look at the new pipe introduced in the development version of R 4.1.0, and compare it to the well-known %\u0026gt;% pipe from the {magrittr} package that is used throughout the {tidyverse}.\nThere was a recent update to {magrittr} which switched to implementing the bulk of the piping in the C language rather than directly in R. Because of this, as well as showing some features of the new base pipe, |\u0026gt;, I’m going to compare it to both the new {magrittr} pipe, %\u0026gt;% and the old version, which I am going to style as %\u0026gt;\u0026gt;%\ninstall.packages(\u0026quot;magrittr\u0026quot;)\rremotes::install_github(\u0026quot;Myko101/magrittrclassic\u0026quot;)\rIf you want to install the classic {magrittr} without this updated %\u0026gt;\u0026gt;% pipe then run remotes::install_github(\"Myko101/magrittrclassic@classic\") to have it loaded as a package called {magrittrclassic} or remotes::install_github(\"tidyverse/magrittr@v1.5) to have it overwrite your current {magrittr} package. Note that this is prone to errors, particularly if {magrittr} or any packages that depend on it are loaded.\nThe first thing to inspect is the speed of this new pipe in a simple situation. Let’s create a simple function and see how it goes in the bench::mark() function\ndoubler \u0026lt;- function(val) 2*val\rx \u0026lt;- 1:10\rbm \u0026lt;- bench::mark(\rstandard = doubler(x),\rmagrittrclassic = x %\u0026gt;\u0026gt;% doubler(),\rmagrittr = x %\u0026gt;% doubler(),\rbase = x |\u0026gt; doubler()\r)\rggplot2::autoplot(bm)\rNote that the `bench::mark()`` function by default also checks whether the results we get are the same.\nThe first thing that jumps out is just how slow the old {magrittr} implementation is and how fast the base/standard versions are. The time scale on the plot is logarithmic, which shows that the old {magrittr} function is almost 2 orders of magnitude slower (800ns vs 72.5 us), that’s nearly 100x slower!\nWhy is this? Firstly, the old {magrittr} pipe builds functions in R and then applies them to data turn by turn. However, the new {magrittr} pipe does all this in C. How is the base version so much faster? Well it is a syntax rather than an infix operator or a call.\nThis means that x %\u0026gt;% f() builds functions and performs actions to produce output which is identical to f(x). However, x |\u0026gt; f() is the same as f(x), it’s just a different way of writing it. Think of using a single quote, ' or a double quote \" to create a string, the command you’re giving to R is different, but the result is parsed identically before any actual R code is ran. Similarly, when you run 2 + 3 + 4, R will parse that as ( (2+3) + 4 )because the addition operator can only run on two objects so R has to divvy them up appropriately (left to right).\nThis can be evidenced by capturing the calls using the rlang::exprs() function\nrlang::exprs(\rstandard = doubler(x),\rmagrittrclassic = x %\u0026gt;\u0026gt;% doubler(),\rmagrittr = x %\u0026gt;% doubler(),\rbase = x |\u0026gt; doubler()\r)\r## $standard\r## doubler(x)\r## ## $magrittrclassic\r## x %\u0026gt;\u0026gt;% doubler()\r## ## $magrittr\r## x %\u0026gt;% doubler()\r## ## $base\r## doubler(x)\rSee the last one there? x |\u0026gt; doubler() is exactly doubler(x). There’s no transforming in R here, it just is the same thing.\nThis functionality is added to by the introduction of a new lambda function creation shortcut, let’s compare it to the {magrittr} implementation(s) of anonymous functions, using the dot notation:\nbm2 \u0026lt;- bench::mark(\rstandard = (function(y) 2*y)(x),\rmagrittrclassic = x %\u0026gt;\u0026gt;% {2*.},\rmagrittr = x %\u0026gt;% {2*.},\rbase = x |\u0026gt; \\(y) 2*y\r)\rggplot2::autoplot(bm2)\rTimings are very similar to the previous one, especially when only looking relatively. The slow down is probably due to the creation of a function in each use, which also explains why they are all around the same amount slower. What do these piped lambda functions look like?\nrlang::exprs(\rstandard = (function(y) 2*y)(x),\rmagrittrclassic = x %\u0026gt;\u0026gt;% {2*.},\rmagrittr = x %\u0026gt;% {2*.},\rbase = x |\u0026gt; \\(y) 2*y\r)\r## $standard\r## (function(y) 2 * y)(x)\r## ## $magrittrclassic\r## x %\u0026gt;\u0026gt;% {\r## 2 * .\r## }\r## ## $magrittr\r## x %\u0026gt;% {\r## 2 * .\r## }\r## ## $base\r## (function(y) 2 * y)(x)\rAgain the standard and base versions are parsed the same.\nOne final critic of the new pipe is that you can only pass an object to the first argument in a function. This is a limitation in a lot of cases, particularly because most {base} functions don’t follow the convention of passing the out current data as the first argumnt. But using the lambda \\\\() syntax, we can get around this. We can also pass named arguments in the same way we usually would when calling a function. Let’s try it and time it\nmultiplier \u0026lt;- function(a,val) a*val\rbm3 \u0026lt;- bench::mark(\rstandard = multiplier(2,x),\rmagrittrclassic = x %\u0026gt;\u0026gt;% multiplier(2,.),\rmagrittr = x %\u0026gt;% multiplier(2,.),\rbase_lambda = x |\u0026gt; \\(y) multiplier(2,y),\rbase_named = x |\u0026gt; multiplier(a=2)\r)\rggplot2::autoplot(bm3)\rClearly, the lambda version of the base packages needs more time, again because it is creating the function in the middle, whereas the named version does not have to do this. Why? Again, let’s capture them:\nrlang::exprs(\rstandard = multiplier(2,x),\rmagrittrclassic = x %\u0026gt;\u0026gt;% multiplier(2,.),\rmagrittr = x %\u0026gt;% multiplier(2,.),\rbase_lambda = x |\u0026gt; \\(y) multiplier(2,y),\rbase_named = x |\u0026gt; multiplier(a=2)\r)\r## $standard\r## multiplier(2, x)\r## ## $magrittrclassic\r## x %\u0026gt;\u0026gt;% multiplier(2, .)\r## ## $magrittr\r## x %\u0026gt;% multiplier(2, .)\r## ## $base_lambda\r## (function(y) multiplier(2, y))(x)\r## ## $base_named\r## multiplier(x, a = 2)\rOne final thing to look at is the lambda function part of this whole process. While the {tidyverse} doesn’t provide a general shortcut to produce these, they can be created within other functions. For example, the above syntax {2*.} only works within the context of a pipe and wouldn’t work as a piece of code on it’s own.\nThe other major way in which lambda functions are declared is through the {purrr} package. The {purrr} package provides methods of functional programming (to an extent), and so within a {purrr} function, we can define a function using the ~ symbol. Let’s compare it to the \\\\() syntax, remember, this is again a syntax and not a function/call!\nlibrary(purrr,warn.conflicts=F)\rbm4 \u0026lt;- bench::mark(\rstandard = {\rres \u0026lt;- vector(\u0026quot;list\u0026quot;,10)\rfor(i in 1:10) res[[i]] \u0026lt;- mean(1:i)\rres\r},\rpurrr = map(1:10,~mean(1:.)),\rbase = lapply(1:10,\\(i) mean(1:i))\r)\rggplot2::autoplot(bm4)\rAgain due to the lack of overheads for the \\\\()`` syntax, speed is definitely on it's side. We could just as easily use thelapply()function here and declare theFUNargument usingfunction(i) mean(1:i), but writing\\()` is much quicker/easier.\n","date":1607212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607263508,"objectID":"f6c5fc86f60f43f538a4b9612edc8187","permalink":"https://michaelbarrowman.co.uk/post/the-new-base-pipe/","publishdate":"2020-12-06T00:00:00Z","relpermalink":"/post/the-new-base-pipe/","section":"post","summary":"Here, we’re going to take a quick look at the new pipe introduced in the development version of R 4.1.0, and compare it to the well-known %\u0026gt;% pipe from the {magrittr} package that is used throughout the {tidyverse}.","tags":[],"title":"The New Base Pipe","type":"post"},{"authors":[],"categories":[],"content":"\r\rIntroduction\rFirst, let’s load up what we need!\nset.seed(15102020)\rlibrary(tidyverse) #We\u0026#39;ll use tidyverse functions\rlibrary(magrittr) #A few extra pipes from magrittr\rlibrary(lexicon) #For a word dictionary\rWhen dealing wth complex datasets, it is common that a variable may be stored as a character variable, when in reality what you want is a factor variable. On the surface, these two constructs look very similar:\neg_df \u0026lt;- tibble(\rc_var = c(\u0026quot;Cat\u0026quot;,\u0026quot;Dog\u0026quot;,\u0026quot;Cat\u0026quot;,\u0026quot;Mouse\u0026quot;,\u0026quot;Mouse\u0026quot;),\rf_var = factor(c(\u0026quot;Cat\u0026quot;,\u0026quot;Dog\u0026quot;,\u0026quot;Cat\u0026quot;,\u0026quot;Mouse\u0026quot;,\u0026quot;Mouse\u0026quot;))\r)\reg_df\r## # A tibble: 5 x 2\r## c_var f_var\r## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt;\r## 1 Cat Cat ## 2 Dog Dog ## 3 Cat Cat ## 4 Mouse Mouse\r## 5 Mouse Mouse\rHowever, underneath they are treated quite differently. Behind the scenes, the factors are actually stored as integers with a special lookup table called their levels, which can be seen if we print the variable individually:\neg_df$f_var\r## [1] Cat Dog Cat Mouse Mouse\r## Levels: Cat Dog Mouse\rWe can also see the hidden numbers by converting this to numeric:\nas.numeric(eg_df$f_var)\r## [1] 1 2 1 3 3\rThe first element, Cat is associated with the first level, so it is stored as a 1, the third element is also Cat, so it is also stored as a 1. The fourth \u0026amp; fifth are both Mouse and so they’re stored as 3, indicating to use the third level.\n\rWhy factors?\rMost statistical operations within R that can act on a character variable will essentially convert to a factor first. So, it’s more efficient to convert characters to factors before passing them into these kinds of functions. This also gives us more control over what we’re going to get.\nThis conversion makes many processes that work with characters a bit slow. If you’re wanting to do 20 functions on a data set and each one needs to convert your characters to factors internally before doing what it needs to, it’s clearly much faster to manually convert once before using these functions.\nFactors also take up slightly less space in your system’s memory. In R, this is approximately half the space of a character, however the way R stores this kind of data is surprisingly efficient. It’s definitely a good habit to get into if you ever want to move onto less efficient storage methods.\n\rConverting\rAbove, I used the factor() function to quickly convert a single character variable to a factor variable. But what about if you’ve got a large dataset with many, many character variables that you want to convert to factors. What’s the smoothest way to do this?\nExample random dataset\rFirst, let’s create a large dataset, we’ll loop through a bunch of columns. We’ll use Fry’s 1000 Most Commonly Use English Words, as found in the sw_fry_1000 dataset from the {lexicon} package to choose random words for each variable. We’ll also throw in some numeric variables to make things harder:\ndf \u0026lt;- tibble(id=1:1000) #declare a tibble with just an id variable\rfor(i in 1:10)\r{\r#How many distinct words should this variable have?\rdistinct_words \u0026lt;- round(rexp(1,1/20)) +1\r#What words can we choose from for this variable?\rthese_words \u0026lt;- sample(sw_fry_1000,distinct_words)\r#What\u0026#39;s the name of this variable?\rthis_name \u0026lt;- paste0(\u0026quot;var_\u0026quot;,ncol(df) + 1)\r#Generate the variable\rthis_variable \u0026lt;- sample(these_words,1000,replace=T)\r#Store it in the tibble\rdf[[this_name]] \u0026lt;- this_variable\r#Approximated 1/3 of the time, we\u0026#39;ll add a numeric variable\rif(rbinom(1,1,1/3) == 1){\rthis_name \u0026lt;- paste0(\u0026quot;var_\u0026quot;,ncol(df)+1)\rdf[[this_name]] \u0026lt;- rnorm(1000)\r}\r}\rdf\r## # A tibble: 1,000 x 14\r## id var_2 var_3 var_4 var_5 var_6 var_7 var_8 var_9 var_10 var_11 var_12\r## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 prop~ 1.83 row gove~ gene~ cow else women length 0.249 home ## 2 2 four -0.225 wind~ reas~ speak cow squa~ gold exerc~ 0.688 numer~\r## 3 3 leave 0.367 gold plant came cow egg human exerc~ -0.517 tell ## 4 4 rock 0.919 that meat gene~ cow leave human skill -0.280 fill ## 5 5 favor -1.01 mile nine tree cow very hand has -0.0302 left ## 6 6 shop 1.14 hunt drink speak cow take meat hit 0.908 over ## 7 7 end 0.0427 engi~ seas~ gene~ cow art women exerc~ 0.0395 unit ## 8 8 favor -0.647 body drink gene~ cow diff~ doll~ most -0.458 people\r## 9 9 earth -2.47 fight nine tree cow deci~ air king 0.0182 child ## 10 10 end 1.35 prot~ drink speak cow carry women grand -0.978 conti~\r## # ... with 990 more rows, and 2 more variables: var_13 \u0026lt;chr\u0026gt;, var_14 \u0026lt;dbl\u0026gt;\rThe generation of this data is actually rather clunky as it’s using a loop, and we’re going to avoid that. Instead, we’re going to turn all these characters into factors in a single line. Here’s the line of code which will update the dataset, followed by the explanation:\n\r\rThe solution\rWith {tidyverse} processes, the key thing we’re trying to do is build a “sentence” explaining what we’re doing. Here’s our expression, followed by the English sentence equivalent\ndf %\u0026lt;\u0026gt;% mutate(across(where(is.character),as_factor))\r#Update the df by mutating it across variables where it is a\r# character by performing as_factor on them\rdf\r## # A tibble: 1,000 x 14\r## id var_2 var_3 var_4 var_5 var_6 var_7 var_8 var_9 var_10 var_11 var_12\r## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt; ## 1 1 prop~ 1.83 row gove~ gene~ cow else women length 0.249 home ## 2 2 four -0.225 wind~ reas~ speak cow squa~ gold exerc~ 0.688 numer~\r## 3 3 leave 0.367 gold plant came cow egg human exerc~ -0.517 tell ## 4 4 rock 0.919 that meat gene~ cow leave human skill -0.280 fill ## 5 5 favor -1.01 mile nine tree cow very hand has -0.0302 left ## 6 6 shop 1.14 hunt drink speak cow take meat hit 0.908 over ## 7 7 end 0.0427 engi~ seas~ gene~ cow art women exerc~ 0.0395 unit ## 8 8 favor -0.647 body drink gene~ cow diff~ doll~ most -0.458 people\r## 9 9 earth -2.47 fight nine tree cow deci~ air king 0.0182 child ## 10 10 end 1.35 prot~ drink speak cow carry women grand -0.978 conti~\r## # ... with 990 more rows, and 2 more variables: var_13 \u0026lt;fct\u0026gt;, var_14 \u0026lt;dbl\u0026gt;\rAnd as if by magic, all of the characters are now factors (note the \u0026lt;fct\u0026gt; under the variable names).\n\rThe Explanation\rThe above code uses five functions, and an operation to perform the action. We’ll dig down into the functions and then climb back out as their results are processed:\n\r%\u0026lt;\u0026gt;% grabs the tibble on it’s left hand side and passes it to the function on the right. At this point, it works exactly like the regular %\u0026gt;% operator\r\rmutate() means we are creating or updating a variable inside the tibble\r\racross() allows us perform a function across many variables within the tibble\r\rwhere() allows us to specify where we want across() to perform the function\r\ris.character(), in the above line, we don’t use the brackets for is.character() because we’re not applying it, we’re referencing it. We’re telling the where() function to use this when checking where we want the function to be applied. The is.character() function returned TRUE when the variable is a character and FALSE when it isn’t (e.g. a numeric)\r\rwhere() therefore applies this function to every variable in df and returns a vector of TRUE and FALSE to across() to indicate which variables in the tibble we want across() to act on\ras_factor() converts things (e.g. characters) into factors.\r\racross() has now been passed a logical vector telling it which columns to apply a function and a function that it needs to apply. So it does just that and outputs another tibble\r\rmutate() has then been passed a tibble for it’s first argument (df via the %\u0026lt;\u0026gt;% pipe) and another tibble as the output of across(). It stitches these together, if there are any names in common, it overwrites those in df with those from across(). All the variables in across() will also appear in df because that’s where they came from, so the old values are overwritten with the new ones\r\r%\u0026lt;\u0026gt;% then receives this new tibble from mutate() and stores it back into the df tibble that we originally passed to it. This is essentially saying that df %\u0026lt;\u0026gt;% f() is the same as df \u0026lt;- df %\u0026gt;% f(), that’s why this is called the assignment pipe or updating pipe.\r\r\r","date":1602720000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602770708,"objectID":"7f9169d35291ee59820987aa3e355f5b","permalink":"https://michaelbarrowman.co.uk/post/convert-all-character-variables-to-factors/","publishdate":"2020-10-15T00:00:00Z","relpermalink":"/post/convert-all-character-variables-to-factors/","section":"post","summary":"Introduction\rFirst, let’s load up what we need!\nset.seed(15102020)\rlibrary(tidyverse) #We\u0026#39;ll use tidyverse functions\rlibrary(magrittr) #A few extra pipes from magrittr\rlibrary(lexicon) #For a word dictionary\rWhen dealing wth complex datasets, it is common that a variable may be stored as a character variable, when in reality what you want is a factor variable.","tags":[],"title":"Convert all Character variables to Factors","type":"post"},{"authors":["Glen Philip Martin","David A Jenkins","Lucy Bull","Rose Sisk","Lijing Lin","William Hulme","Anthony Wilson","Wenjuan Wang","Michael Barrowman","Camilla Sammut-Powell","Alexander Pate","Matthew Sperrin","Niels Peek","Predictive Healthcare Analtics Group"],"categories":null,"content":"","date":1595721600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595721600,"objectID":"07813a380f844215dd90c395b722189b","permalink":"https://michaelbarrowman.co.uk/publication/martin-2020a/","publishdate":"2020-07-26T00:00:00Z","relpermalink":"/publication/martin-2020a/","section":"publication","summary":"__Background and Objective__\n\nIn view of the growth of published articles, there is an increasing need for studies that summarize scientific research. An increasingly common review is a \"methodology scoping review,\" which provides a summary of existing analytical methods, techniques and software that have been proposed or applied in research articles to address an analytical problem or further an analytical approach. However, guidelines for their design, implementation, and reporting are limited.\n\n__Methods__\n\nDrawing on the experiences of the authors, which were consolidated through a series of face-to-face workshops, we summarize the challenges inherent in conducting a methodology scoping review and offer suggestions of best practice to promote future guideline development.\n\n__Results__\n\nWe identified three challenges of conducting a methodology scoping review. First, identification of search terms; one cannot usually define the search terms a priori, and the language used for a particular method can vary across the literature. Second, the scope of the review requires careful consideration because new methodology is often not described (in full) within abstracts. Third, many new methods are motivated by a specific clinical question, where the methodology may only be documented in supplementary materials. We formulated several recommendations that build upon existing review guidelines. These recommendations ranged from an iterative approach to defining search terms through to screening and data extraction processes.\n\n__Conclusion__\n\nAlthough methodology scoping reviews are an important aspect of research, there is currently a lack of guidelines to standardize their design, implementation, and reporting. We recommend a wider discussion on this topic.","tags":null,"title":"Toward a Framework for the Design, Implementation, and Reporting of Methodology Scoping Reviews","type":"publication"},{"authors":[],"categories":[],"content":"\r\rThere is no greater staple of the {tidyverse} than the pipe, %\u0026gt;%, however not a lot of people understand what’s going on “under-the-bonnet” of the pipe. To be fair, not many people have to worry about it. Until you start trying to do a bit of meta-programming. Then things can get difficult.\nRecently, a question posed by user preposterior on RStudio Community embodied an issue that can happen when trying to extract the name of a variable.\nWhen using the {rlang} package, we can get the name of a variable passed into a function using the following:\nsimple_get_name \u0026lt;- function(x){\rx_sym \u0026lt;- ensym(x)\ras_name(x_sym)\r}\rmy_variable \u0026lt;- 1\rsimple_get_name(my_variable)\r## [1] \u0026quot;my_variable\u0026quot;\rHowever, when we try to use this with a pipe, it goes wrong:\nmy_variable %\u0026gt;% simple_get_name()\r## [1] \u0026quot;.\u0026quot;\rWhat is this!? Where did that dot come from? Well, that’s why you’re here. The . can actually be used as a variable name in R, it’s perfectly syntactic, although ill-advised. Many functions (particularly {tidyverse} ones) use the . as a filler for other purposes, the pipe is a big example, but it is also prominent in the map() family of functions in {purrr}.\nThe reason we get this output is because the pipe actually turns your pipeline of functions into a chain of new functions defined something like this:\nfunction(.)\rsimple_get_name(.)\rSo, it’s a chain of wrapper functions around your pipeline’d functions. The functions are direct copies of what you use in your pipeline. For example, if you have a few arguments, the . will be inserted as the first argument. This makes sense as this is what the pipe does (passes your input into the first agrument, unless told otherwise).\nOnce the pipe has this chain of functions, it uses the freduce() function to apply the functions in order to the output of the previous one. You already knew what a pipeline did, know you’ve got a little insight into how.\nSo how do we pull out that my_variable name from within a piped function? Well the problem is that, within the context of that function, that variable is lost. It’s value has been put into the variable ., but that original name is long gone.\nWe can, however, look back over the call-stack where the current function is being evaluated (which is what error-finding functions like traceback() do). Within the pipe, it actually creates a relatively deeply nested set of calls (about 9 calls deep). However, the sys.calls() function can return this stack. Compare for example the following two outputs:\nstack_fun \u0026lt;- function(x){\rsys.calls()\r}\rstack_fun(my_variable)\rmy_variable %\u0026gt;% stack_fun\r## [[1]]\r## stack_fun(my_variable)\r## [[1]]\r## my_variable %\u0026gt;% stack_fun\r## ## [[2]]\r## withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))\r## ## [[3]]\r## eval(quote(`_fseq`(`_lhs`)), env, env)\r## ## [[4]]\r## eval(quote(`_fseq`(`_lhs`)), env, env)\r## ## [[5]]\r## `_fseq`(`_lhs`)\r## ## [[6]]\r## freduce(value, `_function_list`)\r## ## [[7]]\r## withVisible(function_list[[k]](value))\r## ## [[8]]\r## function_list[[k]](value)\r## ## [[9]]\r## stack_fun(.)\rThe first element of this stack will be the initial call, in this case my_variable %\u0026gt;% stack_fun(). This will be a call object and so we can pull out the left-hand side by extracting the second element (the %\u0026gt;% is the first element, and stack_fun is the third). Therefore, the previous function can be written as:\nstacked_get_name \u0026lt;- function(x){\rfirst_call \u0026lt;- sys.calls()[[1]] #get the first entry on the call stack\rlhs \u0026lt;- first_call[[2]] #get the second element of this entry\rz \u0026lt;- rlang::as_name(lhs)\rprint(z)\r}\rmy_variable %\u0026gt;% stacked_get_name()\r## [1] \u0026quot;my_variable\u0026quot;\rIt worked! Brilliant!\nBut, that’s not the end of our tale!\nThis is just looking for the initial call, and isn’t strictly going to seek out where there is a pipe. For example, it wouldn’t work with the following function, since wrap_stacked_get_name() would be at the top of the stack:\nwrap_stacked_get_name \u0026lt;- function(wrapped_var){\rthis_variable \u0026lt;- wrapped_var+1\rthis_variable %\u0026gt;% stacked_get_name\r}\rwrap_stacked_get_name(my_variable)\r## [1] \u0026quot;my_variable\u0026quot;\rThis should return this_variable, but since it’s looking at the initial call, it looks too far back up the call-stack and misses this variable.\nHowever, by inspecting the entire stack for a pipe, we can pull out the most recent (i.e. the lowest) entry that is a pipe, and grab the left-hand side of that call.\nget_lhs \u0026lt;- function(){\rcalls \u0026lt;- sys.calls()\r#pull out the function or operator (e.g. the `%\u0026gt;%`)\rcall_firsts \u0026lt;- lapply(calls,`[[`,1) #check which ones are equal to the pipe\rpipe_calls \u0026lt;- vapply(call_firsts,identical,logical(1),quote(`%\u0026gt;%`))\r#if we have no pipes, then get_lhs() was called incorrectly\rif(all(!pipe_calls)){\rNULL\r} else {\r#Get the most recent pipe, lowest on the pipe_calls \u0026lt;- which(pipe_calls)\rpipe_calls \u0026lt;- pipe_calls[length(pipe_calls)]\r#Get the second element of the pipe call\rthis_call \u0026lt;- calls[[c(pipe_calls,2)]]\r#We need to dig down into the call to find the original\rwhile(is.call(this_call) \u0026amp;\u0026amp; identical(this_call[[1]],quote(`%\u0026gt;%`))){\rthis_call \u0026lt;- this_call[[2]]\r}\rthis_call\r}\r}\rOnce we have the call, getting the lhs of it requires digging down. If we have pipeline, then it’s actually a nested sequence of operators. For example, 2+3+4 makes sense to us, but R can’t add like this, it breaks this down by calculating from left to right, basically it does this (2 + 3) + 4, which is the same as add(add(2,3),4). R does this with the pipe too.\nIf we’re piping a few things together, we write this: my_variable %\u0026gt;% fun1 %\u0026gt;% fun2 %\u0026gt;% fun3, R reads it as this: ((my_variable %\u0026gt;% fun1) %\u0026gt;% fun2) %\u0026gt;% fun3.\nSo we repeatedly check that the current function/operator/call name is a pipe, if it is, grab the second entry (which is what is being piped into the current pipe). If it isn’t, we’ve dug down far enough.\nSo, now that we have that little function, we can re-write our function to check for this first:\nget_name \u0026lt;- function(x){\rlhs \u0026lt;- get_lhs()\rif(is.null(lhs)){\rlhs \u0026lt;- rlang::ensym(x)\r}\ras_name(lhs)\r}\rget_name(my_variable)\r## [1] \u0026quot;my_variable\u0026quot;\rmy_variable %\u0026gt;% get_name\r## [1] \u0026quot;my_variable\u0026quot;\rEureka! Now, let’s check the wrapper function:\nwrap_get_name \u0026lt;- function(wrapped_var){\rthis_variable \u0026lt;- wrapped_var+1\rthis_variable %\u0026gt;% get_name\r}\rwrap_get_name(my_variable)\r## [1] \u0026quot;this_variable\u0026quot;\rmy_variable %\u0026gt;% wrap_get_name\r## [1] \u0026quot;this_variable\u0026quot;\rThis function acts a little strange around fseq functions. But, the results make sense when you think about it.\nfseq_get_name \u0026lt;- . %\u0026gt;% get_name\rThis method of creating a function, where the initial starting value of the pipeline is actually that previously discussed ., this is essentially the same as the previous, wrap_get_name() function:\nfseq_get_name_dummy \u0026lt;- function(.){\r. %\u0026gt;% get_name\r}\rAs usual, we can use this function in one of two ways, either as a regular function with brackets or as a piped function\nfseq_get_name(my_variable)\r## [1] \u0026quot;.\u0026quot;\rLooking at the alternate definition above, this makes sense as a result. The pipeline starts with a .\nmy_variable %\u0026gt;% fseq_get_name\r## [1] \u0026quot;my_variable\u0026quot;\rWhat? This time, it’s returned the value being piped in. But, if we imagine a fseq-style as sticking the pipelines together, then the actual start of this pipeline is the my_variable.\nAnd there we go. You’ve now got a bit more of an understanding of the sys.calls() function and can extract the name of a variable being passed into a pipeline. This is a very basic way of doing it, it doesn’t do nearly enough checks as a function in-production would have to do, but it’s a good start. You could also extract any part of that original pipeline call.\nSidenote for the pro’s out there. This page is written in {rmarkdown} and rendered using {blogdown}. This meant that when I used the sys.calls() function, I actually got a much deeper nesting of calls when rendering in these than in my RStudio application. This is because when rendering, each code chunk is evaluated within another call. For the local render with {rmarkdown}, I had to remove the first 18 calls before the “first” call was the one actually used above. For the {blogdown} render, it is 24. This page has a bunch of hidden code chunks (using eval=F and echo=F) to make the code and the output look seamless.\r","date":1595548800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595546735,"objectID":"a8671e08d68476048a6b1217403caab1","permalink":"https://michaelbarrowman.co.uk/post/getting-a-variable-name-in-a-pipeline/","publishdate":"2020-07-24T00:00:00Z","relpermalink":"/post/getting-a-variable-name-in-a-pipeline/","section":"post","summary":"There is no greater staple of the {tidyverse} than the pipe, %\u0026gt;%, however not a lot of people understand what’s going on “under-the-bonnet” of the pipe. To be fair, not many people have to worry about it.","tags":[],"title":"Getting a variable name in a pipeline","type":"post"},{"authors":[],"categories":[],"content":"Within git, the default branch is usually named master, however in recent times, the negative connotations of that word are coming to the forefront of a lot of people\u0026rsquo;s minds, and so they are wishing to diverge away from that kind of terminology. The simplest change that we can make is to default to the HEAD branch of a repo, which will point towards whatever the actual default branch is for a the repo, whether that is master, main or Captain.\nUnfortunately, this change can be slow, and although resources like GitHub have expressed interest in switching away from the default master, some things are still hardcoded. One of which is the limitations of GitHub Pages deployment. Users can currently choose from one of three options:\n Build website from the master branch Build website from the docs folder in the master branch Build website from the gh-pages branch  The use of master here is hardcoded, and many users currently choose to use the docs folder in the master branch as the location to store their website. Depending on workflows, the other two options might not be possible, or would require huge restructuring of your workflow if you wish to switch from master. For User pages (repos that are \u0026lt;username\u0026gt;.github.io), they can only be built from the master branch; hopefully this will change soon (see here).\nFor example, this blog is written using blogdown, a package for R, which helps to create blogs. In doing so, it creates a static site in a subdirectory called public (by default). You do your work in the parent folder, it generates content in this subfolder. If you wish to use GitHub Pages to publish your site, the recommendations by yihui, the package autho, are to set up your git directory inside this subdirectory. This has the limitation of meaning the content of your parent folder is not backed up to GitHub, and is only stored locally.\nHowever, thanks to GitHub user s0 and their GitHub Action, it\u0026rsquo;s possible to keep your work inside the doc folder on your default branch (whatever it\u0026rsquo;s name may be) and have that folder automatically pushed to the gh-pages branch.\nFor those who don\u0026rsquo;t know, GitHub Actions allows automation when certain events (triggers) occur within your repo. You can try to write your own complicated commands, or use those created by other users within a relatively simple skeleton. We\u0026rsquo;re going to use one of these simple skeletons to utilise s0\u0026rsquo;s work, thus allowing us to use the main branch, rather than the master branch on our repo. In order to use GitHub Actions, we need to add a .yaml file (stands for YAML Ain\u0026rsquo;t Markup Language) within our repo.\nRename the branch The first step in this process would be to actually change the name of our master branch to main (or whatever you choose).\nDirectly on GitHub GitHub doesn\u0026rsquo;t directly support renaming of branches (as far as I know). So, what we need to do is to create a new branch for our repository by clicking on the branches button at the top left of our repo Code page (probably says master right now).\nAnd then type in the name of the new branch (e.g. main). If this branch doesn\u0026rsquo;t exist, you\u0026rsquo;ll be given the option to Create branch: main from 'master'. Click on Settings then Branches and you can change your default branch to the new main branch.\nYou can then delete your master branch, although it might be worth holding on to it for a little while, in case anybody downstream from you is using it, or as a back-up in case something goes wrong!\nGit Bash If you have a local copy of your repo, you can run the following in command line to rename it:\ngit branch -m master main\r If you want to rename the current branch, you can simplify this to be:\ngit branch -m main\r Note you will have to use -M instead of -m if you are renaming a branch and only changing captalisation, e.g. from main to Main.\nA common error when running this command is the following (or something to this effect):\nerror: refname refs/heads/HEAD not found\rfatal: Branch rename failed\r This means you don\u0026rsquo;t have a branch checked out, and so you\u0026rsquo;ll have to create a new branch, but when doing so, you can name it whatever you want\ngit checkout -b main\r After you\u0026rsquo;ve done this locally, you\u0026rsquo;ll have to git push your repo up to GitHub again. However, you\u0026rsquo;ll probably get an error telling you to run the following instead\ngit push --set-upstream origin main\r This will just ensure your new main branch is upstream of the previous master branch.\nThen you\u0026rsquo;ll have to change GitHub\u0026rsquo;s default branch to your new one in the Settings as above.\nGitHub Workflows GitHub Action files are stored in a special directory in your repo, the .github/workflow directory. All we have to do is create a file in this directory, name it something useful and give it the .yaml extension. Sounds simple, and for most people it is. The only limitation is that sometimes, we can\u0026rsquo;t create folders with the . at the start (particularly on Windows). Or at least, we can\u0026rsquo;t create them in the usual Right Click \u0026gt; New \u0026gt; Folder method in Windows Explorer. The simple way is to use Command Line to do it.\nmkdir .github\r The mkdir command makes directories. It\u0026rsquo;s as simple as that.\nAlternatively, you can create this directly on GitHub in the usual manner. Just remember that you will have to git pull any changes you make this way.\nThe YAML The YAML file that we create will look like this:\nname: 'Deploy to gh-pages'\ron:\rpush:\tbranches:\r- main\rpaths:\t- 'docs/**'\rjobs:\rdeploy:\rname: Push docs to gh-pages\rruns-on: ubuntu-latest steps:\r- uses: actions/checkout@main\r- name: Deploy\ruses: s0/git-publish-subdir-action@develop\renv:\rREPO: self\rBRANCH: gh-pages\rFOLDER: docs\rGITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\r Feel free to copy the above directly. If you\u0026rsquo;re using RStudio to manage your repo, you can create a New Text File, save it with the extension .yaml and RStudio will conveniently colour code the file. Otherwise, you can do it in notepad (just make sure the extension sticks).\nWhat does this file do? I don\u0026rsquo;t think it\u0026rsquo;s too important to go into great details here, there is plenty of reading to be done on GitHub Actions. So, just a quick look at the important bits, in case you want to change something. Apart from the name: line, it\u0026rsquo;s split into two parts, the on: and the jobs: parts. Note that in YAML files, whitespace is important, and gives structure: push: is a child of on: and jobs: is the parent of deploy:, but on: and jobs: are siblings.\nThe Triggers The on: part contains information on what events will trigger the event. Here, we\u0026rsquo;re telling GitHub that we want this Action to run when we push our repo to the main branch, only if something has changed in the docs folder (or path). This means that any pushes that happen to other branches will be ignored, and any pushes that don\u0026rsquo;t change our docs folder will also be ignored. The syntax for paths: actually allows you to check for changes to anything that matches this string, so by using 'docs/**', we match anything that starts with 'docs/', i.e. anything within the docs folder. This is useful because we\u0026rsquo;re building our gh-pages branch based solely on what\u0026rsquo;s in docs. If something changes elsewhere in the repo, it doesn\u0026rsquo;t matter (even if you are accessing data in your repo, but outside of your site because those changes will still be pushed to your repo, just don\u0026rsquo;t need to trigger a site rebuild). It also doesn\u0026rsquo;t matter what happens on other branches (such as a development branch) because we\u0026rsquo;re not wanting to build our GitHub pages from them.\nThe Actions The jobs: part contains the actual actions that occurs. You can have many Actions and jobs within he same file, but here we only have one job, which consists of two tasks. deploy: is just the formal name for the job that we\u0026rsquo;re running. If we want more jobs to run, we can give them different names and place them at the same hierarchical starting point as deploy: (i.e. with two spaces in front). Different jobs will run in parallel, each individual job will run in order.\nWe then give a bit of information about the job, first it\u0026rsquo;s name Push docs to gh-pages (more like a title), followed by what operating system we want GitHub Actions to use to implement it. Finally, we have the steps:, which is where we put the list of tasks that need to be run (in order).\nThis Action only has two steps and they are both uses: steps, which basically means we\u0026rsquo;re going to be using Actions that are properly defined elsewhere. We could write an action directly here in quotes and supply the name of what application we want it to be run in (at a Command Line level), but we don\u0026rsquo;t have to since these Actions are defined for us. Each task starts with a - followed by the information for that task.\nCheckout The first task is simple - uses: actions/checkout@master. You may recognise the format of this as it comes up a lot within GitHub, it is \u0026lt;user\u0026gt;/\u0026lt;repo\u0026gt;@\u0026lt;branch\u0026gt;. This is because all Actions created by other users, are actually repos. So what we\u0026rsquo;re doing here is saying we want to use the Action defined within the main branch of the checkout repo made by the user actions and we can actually view that repo here, or since it is an Action, we can view it on the Actions Marketplace here.\nThis Action essentially runs a git checkout command on your repo so that it\u0026rsquo;s files can be accessed by your workflow. Actions that change your repo in some way will typically start with this. They will usually also end with something that commits and pushes the results back onto your repo. We don\u0026rsquo;t need to do this part because it is covered by the second task\nDeploy Task number two is where the magic happens. There\u0026rsquo;s a lot more here than in the first task\n- name: Deploy\ruses: s0/git-publish-subdir-action@develop\renv:\rREPO: self\rBRANCH: gh-pages\rFOLDER: docs\rGITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\r We have three children within this task, name:, uses: and env: and env: even has some children of it\u0026rsquo;s own. Firstly, we\u0026rsquo;re giving this task a name, Deploy; this isn\u0026rsquo;t necessary, but it looks a little neater and makes it clearer what this task is doing (useful if you\u0026rsquo;re running a lot of tasks in a single job).\nThe next child is the same as previously, uses: s0/git-publish-subdir-action@develop. We\u0026rsquo;re going to run the action on the develop branch of the git-publish-subdir-action repo by the user s0. Once again, you can view this repo here or as an Action on the Marketplace here. This is main part of what we\u0026rsquo;re doing. This Action does the actual copying of the subdirectory and pushes it to a new branch.\nThe last child is env: and this is where you might have to change things depending on your use-case. This has four children, which are actually variables. Just like in most programming, we work within an environment that contains variables, well here we\u0026rsquo;re going to define some for the git-publish-subdir-action to use.\nYou don\u0026rsquo;t need to worry about REPO and GITHUB_TOKEN, these just mean that the action is going to run on the current repo (REPO: self) and provides authentication that it\u0026rsquo;s really us doing the changes (by generating a GITHUB_TOKEN to use as an auth token). The other two variables are important, it\u0026rsquo;s telling the Action what directory we want to copy, which by default (if you\u0026rsquo;re been running your GitHub Pages using the master/docs form) is currently set to be docs, but this can be any other folder (or sub-folder) in your repo, e.g public/home, my-gh-pages-site or \u0026quot;My Homepage\u0026quot; (don\u0026rsquo;t forget the quotes). Then finally, the name of the branch we want to put it on. If you\u0026rsquo;re looking here with the intention of using a GitHub Page, then this will have to be gh-pages (unless using a User account), but can be any name you want your new branch to be.\nCheck it works Finally, once we\u0026rsquo;ve done all this, we can git push to the main branch of our repo on GitHub and it should build our website (provided we have the GitHub pages set to use gh-pages). To check whether this has worked, simply load up your GitHub Page. You can also have a look at the run through of the Action in the Actions tab in your repo home. This gives you the output of the Command Line of every step of your Action (although remember there are only two).\nUser repos User repos are named like \u0026lt;username\u0026gt;.github.io and can only be built from the master branch. This is because User repos are expected to be self-contained and their GitHub Pages site can only be built from the master branch. This means that you are expected to have an index.html file in your home directory. However, this can limit the ways in which your site can be built. To fix this, we can use the workflow as above, but instead of pushing the subdirectory to the gh-pages branch, we push it to the master branch, whilst still using our main branch as our default. The only change we need to do, is to replace the BRANCH: gh-pages line in the YAML file with BRANCH: master (make sure you keep the whitespace before it).\nIf you are using a Custom Domain, there is one last thing that you\u0026rsquo;ll need to consider. When you add a Custom Domain, GitHub stores this as the CNAME file in your home directory. The Action above destroys the master branch before rebuilding it from scratch. This includes deleting that CNAME file and, since it isn\u0026rsquo;t in the subdirectory of your main branch, it doesn\u0026rsquo;t get put back in. The solution? Just put it in there yourself. This simply means adding a file to your docs directory called CNAME (no file extension) and have it\u0026rsquo;s only contents be your url. Since new files without extensions can, again, be tricky. At Command Line:\necho -n [YOUR_URL_HERE] \u0026gt; docs/CNAME\r (The use of -n here means that a newline isn\u0026rsquo;t added to the end of the line)\n","date":1595376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595412242,"objectID":"480d77140dd6b32aecb0d34fa37c699e","permalink":"https://michaelbarrowman.co.uk/post/deploy-to-github-pages/","publishdate":"2020-07-22T00:00:00Z","relpermalink":"/post/deploy-to-github-pages/","section":"post","summary":"Within git, the default branch is usually named master, however in recent times, the negative connotations of that word are coming to the forefront of a lot of people\u0026rsquo;s minds, and so they are wishing to diverge away from that kind of terminology.","tags":[],"title":"Deploy to GitHub Pages","type":"post"},{"authors":["Michael Barrowman","Niels Peek","Mark Lambie","Glen Philip Martin","Matthew Sperrin"],"categories":null,"content":"","date":1564531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564531200,"objectID":"17682ac75413d2cd4d1e1893c411e00d","permalink":"https://michaelbarrowman.co.uk/publication/barrowman-2019a/","publishdate":"2019-07-31T00:00:00Z","relpermalink":"/publication/barrowman-2019a/","section":"publication","summary":"__Background__\n\nAnalysis of competing risks is commonly achieved through a cause specific or a subdistribution framework using Cox or Fine \u0026 Gray models, respectively. The estimation of treatment effects in observational data is prone to unmeasured confounding which causes bias. There has been limited research into such biases in a competing risks framework.\n\n__Methods__\n\nWe designed simulations to examine bias in the estimated treatment effect under Cox and Fine \u0026 Gray models with unmeasured confounding present. We varied the strength of the unmeasured confounding (i.e. the unmeasured variable's effect on the probability of treatment and both outcome events) in different scenarios.\n\n__Results__\n\nIn both the Cox and Fine \u0026 Gray models, correlation between the unmeasured confounder and the probability of treatment created biases in the same direction (upward/downward) as the effect of the unmeasured confounder on the event-of-interest. The association between correlation and bias is reversed if the unmeasured confounder affects the competing event. These effects are reversed for the bias on the treatment effect of the competing event and are amplified when there are uneven treatment arms.\n\n__Conclusion__\n\nThe effect of unmeasured confounding on an event-of-interest or a competing event should not be overlooked in observational studies as strong correlations can lead to bias in treatment effect estimates and therefore cause inaccurate results to lead to false conclusions. This is true for cause specific perspective, but moreso for a subdistribution perspective. This can have ramifications if real-world treatment decisions rely on conclusions from these biased results. Graphical visualisation to aid in understanding the systems involved and potential confounders/events leading to sensitivity analyses that assumes unmeasured confounders exists should be performed to assess the robustness of results.","tags":null,"title":"How Unmeasured Confounding in a Competing Risks Setting Can Affect Treatment Effect Estimates in Observational Studies","type":"publication"},{"authors":["Alexander Pate","Michael Barrowman","David Webb","Jeanne M Pimenta","Kourtney J Davies","Rachael Williams","Tjeerd Van Staa","Matthew Sperrin"],"categories":null,"content":"","date":1540425600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540425600,"objectID":"2837f6b65285ddb7cd8bbc2884ea9347","permalink":"https://michaelbarrowman.co.uk/publication/pate-2018a/","publishdate":"2018-10-25T00:00:00Z","relpermalink":"/publication/pate-2018a/","section":"publication","summary":"__Introduction__\n\nTraditional phase IIIb randomised trials may not reflect routine clinical practice. The Salford Lung Study in chronic obstructive pulmonary disease (SLS COPD) allowed broad inclusion criteria and followed patients in routine practice. We assessed whether SLS COPD approximated the England COPD population and evidence for a Hawthorne effect.\n\n__Methods__\n\nThis observational cohort study compared patients with COPD in the usual care arm of SLS COPD (2012-2014) with matched non-trial patients with COPD in England from the Clinical Practice Research Datalink database. Generalisability was explored with baseline demographics, clinical and treatment variables; outcomes included COPD exacerbations in adjusted models and pretrial versus peritrial comparisons.\n\n__Results__\n\nTrial participants were younger (mean, 66.7 vs 71.1 years), more deprived (most deprived quintile, 51.5% vs 21.4%), more current smokers (47.5% vs 32.1%), with more severe Global initiative for chronic Obstructive Lung Disease stages but less comorbidity than non-trial patients. There were no material differences in other characteristics. Acute COPD exacerbation rates were high in the trial population (98.37th percentile).\n\n__Conclusion__\n\nThe trial population was similar to the non-trial COPD population. We observed some evidence of a Hawthorne effect, with more exacerbations recorded in trial patients; however, the largest effect was observed through behavioural changes in patients and general practitioner coding practices.","tags":null,"title":"Study Investigating the Generalisability of a COPD Trial Based in Primary Care (Salford Lung Study) and the Presence of a Hawthorne Effect","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c9b5771543b03b8149b612b630936a56","permalink":"https://michaelbarrowman.co.uk/experience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/experience/","section":"","summary":"","tags":null,"title":"Experience","type":"widget_page"}]